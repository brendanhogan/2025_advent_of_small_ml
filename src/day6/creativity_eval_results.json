{
  "config": {
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "steering_weight": 0.1,
    "seed": 42,
    "num_pairs_part1": 5,
    "num_rounds_part2": 10,
    "num_rounds_part3": 10
  },
  "part1": {
    "abstracts": [
      {
        "type": "base",
        "seed": 42,
        "abstract": "Abstract:\n\nWe present a novel framework, termed **Neural Uncertainty Estimation and Calibration (NUEC)**, which significantly advances the state-of-the-art in probabilistic prediction and uncertainty quantification for deep learning models. Our approach leverages recent advancements in Bayesian neural networks and deep ensemble methods to develop a robust and scalable method for estimating predictive uncertainty in high-dimensional, complex datasets. Key contributions include a novel architecture that dynamically adjusts the model's uncertainty estimation based on the input data's characteristics, and a novel calibration technique that ensures the predictive probabilities match the true frequencies, thus providing reliable uncertainty estimates.\n\nExperiments on a diverse set of benchmark datasets, including ImageNet, CIFAR-10, and various time-series forecasting tasks, demonstrate that NUEC outperforms existing methods in both accuracy and uncertainty calibration, achieving up to a 20% improvement in average calibration error across multiple metrics. Furthermore, we provide theoretical guarantees on the robustness and generalizability of our method, supported by extensive empirical validation. Our framework is generalizable to a wide range of deep learning architectures and applications, making it a versatile tool for improving the reliability and interpretability of machine learning models in critical applications such as healthcare, finance, and autonomous systems.\n\nThis work not only pushes the boundaries of what is possible in probabilistic predictive modeling but also provides a practical, generalizable solution that can significantly enhance the deployment of machine learning systems in real-world, high-stakes environments. We believe that NUEC will play a pivotal role in advancing the field of machine learning by providing more reliable and interpretable predictions, thus contributing to the development of more trustworthy AI systems."
      },
      {
        "type": "steering",
        "seed": 43,
        "steering_weight": 0.1,
        "abstract": "**Abstract:**\n\nWe introduce a novel framework, **Generative Flow Diffusion**, that bridges the worlds of score-based generative models and flows. Our key insight is to treat the diffusion process as an exact gradient flow for a potential function with a carefully chosen functional form. Building on this, we derive an efficient implicit update for the diffusion process that we call **Score-Diffusion-Flow**, that avoids an iterative implicit update step. We give an exact and efficient formula to compute the diffusion target log-density, enabling an implicit update that is fast and scalable. We develop a powerful score correction procedure that uses an exact implicit formula for the score function. Together, these yield a method that can generate at full resolution and high fidelity at 100x lower FLOPs and 20x lower VRAM than state-of-the-art score-based methods. We demonstrate an order of magnitude larger model size, or equivalently, generation at twice the resolution, at the same compute and memory cost. We further train models up to 300 billion parameters, and demonstrate a 6x speed up over SDE reverse diffusion flows. Our method can make fast progress at full resolution, and we demonstrate a diffusion run time of 1 second at full resolution (vs 15 minutes at the prior SOTA). In our ablations we show that these two key ideas, the score function and the target density formula, are nearly sufficient for all of the observed improvements. The framework is broadly applicable to a wide range of flows and score-based models, and our code, checkpoints and pretrained models are available at https://github.com/kongminjie/gfdiffusion."
      },
      {
        "type": "base",
        "seed": 44,
        "abstract": "Abstract:\n\nThis paper introduces a novel framework, named Adversarial Robustness via Data Augmentation and Transfer Learning (ARDAT), designed to significantly enhance the adversarial robustness of deep neural networks (DNNs) across multiple domains. ARDAT leverages a two-pronged approach that integrates data augmentation techniques tailored to generate adversarial examples and a transfer learning strategy to adapt these augmented datasets for improved robustness. Technically rigorous experiments on benchmark datasets, including MNIST, CIFAR-10, and ImageNet, demonstrate that ARDAT achieves state-of-the-art performance in adversarial robustness while maintaining or even improving standard test accuracy. Notably, ARDAT shows robustness against a wide array of adversarial attacks, including the popular PGD and FGSM, and is highly scalable to large datasets and complex models. The framework's effectiveness is further validated through comprehensive ablation studies and a detailed analysis of its mechanisms. ARDAT opens new avenues for the design of more secure and reliable machine learning models in adversarial environments, making significant contributions to the field of robust machine learning."
      },
      {
        "type": "steering",
        "seed": 45,
        "steering_weight": 0.1,
        "abstract": "**Abstract:**\n\nIncorporating Large Language Models into Autoregressive Models: A Unified Framework for Modeling Text, Code, and Math\n\nDespite the dramatic improvement in generative performance from large language models (LLMs), existing autoregressive (AR) models such as language models and text-to-text conditional models cannot utilize them. We propose a unified framework, UnifiedAR (U-AR), which seamlessly fuses the existing AR model and a very large language model (LLM) into a single generative model. We condition the output of the base AR model on the output of the large LLM, creating an autoregressive generative process conditioned on the very LLM that was never used as a conditioning. We theoretically justify why using the autoregressive base model conditioned on the very LLM is favorable, and we show how to combine the two into a single generative model at inference. We evaluate U-AR on text generation and text-to-code generation (two typical use-cases of text-to-text conditional AR models) on a total of 21 datasets, and our model achieves SOTA or SOTA+ on all datasets. On a total of 24 math formula generation datasets, our model achieves the SOTA or SOTA+ on 21 out of 24 datasets, while the existing models fail on 12. The same unified framework also works when U-AR is given some (very large) text tokens as a prefix. If the prefix is LLM tokens, U-AR (or the LLM) will make it almost act the same as the base model with the prefix as a conditioning. We release our code and model at https://github.com/bozhao-xi/unified-ar."
      },
      {
        "type": "base",
        "seed": 46,
        "abstract": "**Abstract:**\n\nThis paper introduces a novel framework for learning robust and interpretable representations in deep neural networks, leveraging a novel meta-learning approach to enhance model generalization across diverse and complex datasets. Our proposed method, Meta-Inductive Learning with Invariant Representations (MILIR), dynamically adapts the network architecture based on the underlying invariances of the task at hand, ensuring that the learned representations are not only task-specific but also invariant to irrelevant transformations. By integrating a meta-learning mechanism that explicitly encourages invariance, MILIR outperforms state-of-the-art methods on a suite of challenging benchmark datasets, including image classification, object detection, and few-shot learning tasks. Importantly, MILIR provides significant interpretability, allowing for the visualization and analysis of learned invariances, which is crucial for understanding and debugging deep learning models. We also provide extensive experimental evaluations, including ablation studies and comparative analyses, demonstrating the robustness and generalizability of our approach. This work not only advances the state-of-the-art in deep learning but also paves the way for more interpretable and robust AI systems, addressing critical challenges in real-world applications."
      },
      {
        "type": "steering",
        "seed": 47,
        "steering_weight": 0.1,
        "abstract": "We present *Model-Assisted Learning*, a new approach for training discriminative language models from text alone that can achieve zero-shot performance in downstream tasks with no task-specific tuning. Unlike existing pretrained language models, our method can learn task-specific language representations (i.e., the `model`) without the need to optimize the discriminative `head` on a downstream task. We achieve this by pre-training the `model` with an auxiliary task that can only be solved with the full language model capacity. The `head` then just learns to discriminate the language model's representations on a downstream task. We show experimentally that this simple training procedure avoids over-fitting the `head`, the `head` is never asked to learn to do too much, and the `model` can instead have much more capacity. Our approach yields the first pretrained language model that can achieve zero-shot zero-tune performance in downstream language tasks, including summarization, entailment, and fact-checking, that is close to the performance of the fine-tuned model. We believe this approach can lead to models that are more interpretable, with a simple `head` and less to optimize, that will also make pretrained language models more interpretable and debuggable."
      },
      {
        "type": "base",
        "seed": 48,
        "abstract": "Abstract: \n\nIn this paper, we introduce **NeuScape: A Neural Scene Renderer for Efficient and Accurate 3D Scene Understanding**, which addresses the limitations of existing approaches in 3D scene representation and understanding. NeuScape leverages a novel differentiable renderer based on differentiable path tracing, enabling efficient and accurate reconstruction of complex scenes from sparse and noisy observations. Our approach combines the strengths of neural representations and differentiable rendering, offering a unified framework for 3D scene understanding tasks such as surface reconstruction, depth estimation, and semantic segmentation. We demonstrate the superiority of NeuScape over state-of-the-art methods on a variety of benchmarks, including synthetic and real-world datasets. Notably, NeuScape achieves near-perfect surface reconstruction even from extremely sparse point cloud data, showcasing its robustness and efficiency. The paper also provides a comprehensive analysis of the algorithm's robustness to different types of noise and complexity in the input data. We believe NeuScape will have a significant impact on the field of 3D scene understanding, paving the way for more efficient and accurate real-time applications in robotics, autonomous driving, and virtual/augmented reality."
      },
      {
        "type": "steering",
        "seed": 49,
        "steering_weight": 0.1,
        "abstract": "**Abstract:**\n\nWe present a novel and theoretically rigorous framework to train high-quality large language models using a surprisingly simple and cheap method: *a language model trained to predict the last 4 tokens, conditioned on the first 96 tokens of a reference text, can be used to generate the first 97 tokens of the reference text* (we dub the latter a \"prompt\") with nearly the same quality as a reference language model trained from scratch. This is a surprising and theoretically interesting result: it implies that many of the most complex language models in the world are trained using a surprisingly cheap (2 tokens, or 2%) conditioned prediction example, yet in the end, the full language model trained from scratch and the model conditioned on 96 tokens are nearly equal quality when asked to produce 97 tokens, as is the case for many of the largest language models, at least. We also find that models with more tokens sometimes have worse properties (like safety) than those with fewer tokens, suggesting that token-efficiency, rather than just absolute tokens, is important to language models. We find that many of the largest language models, e.g., LLaMA, ChatGPT, or ALiBi, when trained this way, have the wrong properties (like safety or sample diversity) even after being trained for over a day. We train 13B, 70B, 175B, 650B tokens to verify these claims, and the results are similar (except for 13B). We hope to have a better understanding of when we get unsafe, toxic, hallucinated, etc. responses from our models, even when they are trained to be a simple, cheap model. We also suggest a new way to think about the \"training sample\" for a model as a kind of \"prompt\" on how to act, instead of a \"training sample\" as a list of tokens to memorize. Our work shows the surprisingly small size of the prompt to get the training sample is important, e.g., 2 token prompts are important instead of a few sentence instructions, and we want to understand how much and which tokens in the prompt are important."
      },
      {
        "type": "base",
        "seed": 50,
        "abstract": "Abstract:\n\nWe introduce a novel framework, Adaptive Graph Convolutional Networks (AGCN), that dynamically adjusts the graph structure during training to adapt to the underlying data distribution. Unlike static graph approaches, AGCN employs an attention mechanism to learn the most relevant graph connections, significantly improving the model's ability to capture complex dependencies in non-stationary and high-dimensional data. Our framework seamlessly integrates with existing graph convolutional networks, offering a flexible and scalable solution for a wide range of applications including time-series analysis, recommendation systems, and biological networks.\n\nOur experimental evaluations on benchmark datasets from diverse domains, such as financial time series, social networks, and protein-protein interaction graphs, demonstrate that AGCN outperforms state-of-the-art methods by up to 20% in terms of prediction accuracy. We further analyze the learned graph structures and find that AGCN effectively captures both global and local dependencies, leading to more robust and interpretable models. The proposed framework not only addresses the limitations of fixed graph structures but also opens new avenues for exploring dynamic graph learning in various AI applications.\n\nThis work represents a significant advancement in the field of graph neural networks, providing a practical and theoretically grounded solution to the problem of adaptivity in graph-based models. Our contributions are technically rigorous, with clear demonstrations of superior performance and wide applicability, making AGCN a compelling candidate for the best paper award at major AI conferences such as ICML, ICLR, and NeurIPS."
      },
      {
        "type": "steering",
        "seed": 51,
        "steering_weight": 0.1,
        "abstract": "**Abstract:**\n\nDespite decades of progress, machine learning models still suffer from catastrophic forgetting when repeatedly trained on tasks sequentially. Previous efforts to reduce catastrophic forgetting have focused on retaining or replaying the old training data. We take a different route, proposing a novel approach, *Model-Free Retargeting* (MFR), that instead targets the *old model weights*, rather than the old training data. MFR re-weights the current training data such that the current model, when trained on the re-weighted data, would match the old model's predictions. We demonstrate that MFR requires only a few seconds to re-target models pre-trained on a gigabyte of data, and it can substantially reduce catastrophic forgetting, improving over the best existing approaches. Our code is publicly available, open-sourced at https://github.com/google-research/MFR. \n\nThe key technical contribution is showing that MFR is both sample- and compute-efficient. MFR only needs to re-weight the current training data, requiring no access to the old data, and MFR trains the current model on a re-weighted batch, requiring no compute for the old model, which could be prohibitively compute expensive. MFR thus trains current models faster and at a much lower compute cost than previous approaches. We then use this re-weighting idea to directly re-target the old model, making it match the current model. We prove that re-targeting the old model exactly matches the current model, and it only requires a batch of current model predictions. We show that when used to directly re-target the old model, MFR can also outperform existing approaches, for a cheap per-example cost. Both MFR and re-targeting the old model avoid using the old data. To our knowledge, this is the first approach to avoid using the old data, and our method is the most sample-efficient, compute-efficient, and sample-efficient to re-target the old model with existing methods. Our theory and empirical results demonstrate how MFR is efficient and can be broadly applied to substantially outperform the best existing methods that require substantially more samples and compute."
      }
    ]
  },
  "part2": {
    "results": [
      {
        "real_abstract": {
          "title": "Safety Alignment Should be Made More Than Just a Few Tokens Deep",
          "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep."
        },
        "comparisons": [
          {
            "round": 1,
            "base_abstract": "Abstract:\n\nWe present a novel framework, termed **Neural Uncertainty Estimation and Calibration (NUEC)**, which significantly advances the state-of-the-art in probabilistic prediction and uncertainty quantification for deep learning models. Our approach leverages recent advancements in Bayesian neural networks and deep ensemble methods to develop a robust and scalable method for estimating predictive uncertainty in high-dimensional, complex datasets. Key contributions include a novel architecture that dynamically adjusts the model's uncertainty estimation based on the input data's characteristics, and a novel calibration technique that ensures the predictive probabilities match the true frequencies, thus providing reliable uncertainty estimates.\n\nExperiments on a diverse set of benchmark datasets, including ImageNet, CIFAR-10, and various time-series forecasting tasks, demonstrate that NUEC outperforms existing methods in both accuracy and uncertainty calibration, achieving up to a 20% improvement in average calibration error across multiple metrics. Furthermore, we provide theoretical guarantees on the robustness and generalizability of our method, supported by extensive empirical validation. Our framework is generalizable to a wide range of deep learning architectures and applications, making it a versatile tool for improving the reliability and interpretability of machine learning models in critical applications such as healthcare, finance, and autonomous systems.\n\nThis work not only pushes the boundaries of what is possible in probabilistic predictive modeling but also provides a practical, generalizable solution that can significantly enhance the deployment of machine learning systems in real-world, high-stakes environments. We believe that NUEC will play a pivotal role in advancing the field of machine learning by providing more reliable and interpretable predictions, thus contributing to the development of more trustworthy AI systems.",
            "steering_abstract": "**Abstract:**\n\nWe introduce a novel framework, **Generative Flow Diffusion**, that bridges the worlds of score-based generative models and flows. Our key insight is to treat the diffusion process as an exact gradient flow for a potential function with a carefully chosen functional form. Building on this, we derive an efficient implicit update for the diffusion process that we call **Score-Diffusion-Flow**, that avoids an iterative implicit update step. We give an exact and efficient formula to compute the diffusion target log-density, enabling an implicit update that is fast and scalable. We develop a powerful score correction procedure that uses an exact implicit formula for the score function. Together, these yield a method that can generate at full resolution and high fidelity at 100x lower FLOPs and 20x lower VRAM than state-of-the-art score-based methods. We demonstrate an order of magnitude larger model size, or equivalently, generation at twice the resolution, at the same compute and memory cost. We further train models up to 300 billion parameters, and demonstrate a 6x speed up over SDE reverse diffusion flows. Our method can make fast progress at full resolution, and we demonstrate a diffusion run time of 1 second at full resolution (vs 15 minutes at the prior SOTA). In our ablations we show that these two key ideas, the score function and the target density formula, are nearly sufficient for all of the observed improvements. The framework is broadly applicable to a wide range of flows and score-based models, and our code, checkpoints and pretrained models are available at https://github.com/kongminjie/gfdiffusion.",
            "winner": "abstract_b",
            "explanation": "While both abstracts present technically rigorous and impactful advancements, Abstract B stands out for its depth of human creativity, originality, and potential field impact. Instead of incremental improvements, Abstract B proposes a fundamentally new framework\u2014Generative Flow Diffusion\u2014that conceptually unifies score-based generative models and flows through the mathematical notion of gradient flow. The introduction of exact implicit updates for the score function and log-density, enabling dramatic reductions in computational cost (100x lower FLOPs, 20x lower VRAM, and order-of-magnitude larger model sizes), signals not only technical originality but also significant practical effect. The clear reporting of unprecedented performance\u2014up to 300B parameters at much faster run times\u2014demonstrates field-moving impact. The writing is precise and establishes a creative conceptual bridge between disparate areas with a well-articulated vision. In comparison, Abstract A, while solid and generalizable, follows more directly from established techniques (Bayesian NNs, ensembles) and improves them in well-understood ways, lacking the more daring, generative leap of Abstract B. Therefore, Abstract B exhibits a higher degree of genuine creativity and offers a more transformative impact.",
            "was_flipped": false,
            "base_seed": 42,
            "steering_seed": 43
          },
          {
            "round": 2,
            "base_abstract": "Abstract:\n\nThis paper introduces a novel framework, named Adversarial Robustness via Data Augmentation and Transfer Learning (ARDAT), designed to significantly enhance the adversarial robustness of deep neural networks (DNNs) across multiple domains. ARDAT leverages a two-pronged approach that integrates data augmentation techniques tailored to generate adversarial examples and a transfer learning strategy to adapt these augmented datasets for improved robustness. Technically rigorous experiments on benchmark datasets, including MNIST, CIFAR-10, and ImageNet, demonstrate that ARDAT achieves state-of-the-art performance in adversarial robustness while maintaining or even improving standard test accuracy. Notably, ARDAT shows robustness against a wide array of adversarial attacks, including the popular PGD and FGSM, and is highly scalable to large datasets and complex models. The framework's effectiveness is further validated through comprehensive ablation studies and a detailed analysis of its mechanisms. ARDAT opens new avenues for the design of more secure and reliable machine learning models in adversarial environments, making significant contributions to the field of robust machine learning.",
            "steering_abstract": "**Abstract:**\n\nIncorporating Large Language Models into Autoregressive Models: A Unified Framework for Modeling Text, Code, and Math\n\nDespite the dramatic improvement in generative performance from large language models (LLMs), existing autoregressive (AR) models such as language models and text-to-text conditional models cannot utilize them. We propose a unified framework, UnifiedAR (U-AR), which seamlessly fuses the existing AR model and a very large language model (LLM) into a single generative model. We condition the output of the base AR model on the output of the large LLM, creating an autoregressive generative process conditioned on the very LLM that was never used as a conditioning. We theoretically justify why using the autoregressive base model conditioned on the very LLM is favorable, and we show how to combine the two into a single generative model at inference. We evaluate U-AR on text generation and text-to-code generation (two typical use-cases of text-to-text conditional AR models) on a total of 21 datasets, and our model achieves SOTA or SOTA+ on all datasets. On a total of 24 math formula generation datasets, our model achieves the SOTA or SOTA+ on 21 out of 24 datasets, while the existing models fail on 12. The same unified framework also works when U-AR is given some (very large) text tokens as a prefix. If the prefix is LLM tokens, U-AR (or the LLM) will make it almost act the same as the base model with the prefix as a conditioning. We release our code and model at https://github.com/bozhao-xi/unified-ar.",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates a higher level of genuine human creativity and impact. It proposes an original and technically deep concept: a unified framework that combines autoregressive models and large language models in a novel way, with theoretical justifications and robust empirical results across diverse domains (text, code, math). The methodology of conditioning an autoregressive process on the outputs of an LLM is innovative, and the unification across different tasks is both creative and strategically significant for the advancement of generative modeling. \n\nIn contrast, while Abstract B presents a strong and technically rigorous approach to adversarial robustness\u2014which is important and impactful\u2014the methods (data augmentation and transfer learning) are well-explored areas in machine learning and adversarial defense. The integration described, though useful and competitive, does not reflect the same degree of conceptual novelty or potential for broad paradigm shift as seen in Abstract A. \n\nAbstract A's clarity, the breadth and novelty of its proposed framework, and the significant empirical improvements it demonstrates make it the winner on the axes of creativity, technical depth, and potential impact.",
            "was_flipped": true,
            "base_seed": 44,
            "steering_seed": 45
          },
          {
            "round": 3,
            "base_abstract": "**Abstract:**\n\nThis paper introduces a novel framework for learning robust and interpretable representations in deep neural networks, leveraging a novel meta-learning approach to enhance model generalization across diverse and complex datasets. Our proposed method, Meta-Inductive Learning with Invariant Representations (MILIR), dynamically adapts the network architecture based on the underlying invariances of the task at hand, ensuring that the learned representations are not only task-specific but also invariant to irrelevant transformations. By integrating a meta-learning mechanism that explicitly encourages invariance, MILIR outperforms state-of-the-art methods on a suite of challenging benchmark datasets, including image classification, object detection, and few-shot learning tasks. Importantly, MILIR provides significant interpretability, allowing for the visualization and analysis of learned invariances, which is crucial for understanding and debugging deep learning models. We also provide extensive experimental evaluations, including ablation studies and comparative analyses, demonstrating the robustness and generalizability of our approach. This work not only advances the state-of-the-art in deep learning but also paves the way for more interpretable and robust AI systems, addressing critical challenges in real-world applications.",
            "steering_abstract": "We present *Model-Assisted Learning*, a new approach for training discriminative language models from text alone that can achieve zero-shot performance in downstream tasks with no task-specific tuning. Unlike existing pretrained language models, our method can learn task-specific language representations (i.e., the `model`) without the need to optimize the discriminative `head` on a downstream task. We achieve this by pre-training the `model` with an auxiliary task that can only be solved with the full language model capacity. The `head` then just learns to discriminate the language model's representations on a downstream task. We show experimentally that this simple training procedure avoids over-fitting the `head`, the `head` is never asked to learn to do too much, and the `model` can instead have much more capacity. Our approach yields the first pretrained language model that can achieve zero-shot zero-tune performance in downstream language tasks, including summarization, entailment, and fact-checking, that is close to the performance of the fine-tuned model. We believe this approach can lead to models that are more interpretable, with a simple `head` and less to optimize, that will also make pretrained language models more interpretable and debuggable.",
            "winner": "abstract_b",
            "explanation": "Abstract A stands out as more truly human creative and impactful for several reasons. First, it introduces a **fundamentally new approach**\u2014Model-Assisted Learning\u2014that addresses a significant limitation of current discriminative language models: the reliance on task-specific fine-tuning of classification heads. The novel idea of pre-training with an auxiliary task that leverages the full model capacity, followed by a simplistic and less overfit-prone head, is both original and technically deep. It represents a conceptual shift in how zero-shot performance can be achieved, promising not only practical advances but also deeper theoretical understanding and higher interpretability.\n\nIn contrast, while Abstract B proposes a meta-learning framework (MILIR) and emphasizes invariance and interpretability, it builds more incrementally on the existing body of meta-learning literature. Its technical contributions, while useful and well-presented, appear to be refinements or integration of known concepts (meta-learning, architectural adaptation, invariance), rather than a truly new paradigm.\n\nMoreover, Abstract A is clearly written, carefully defining its innovations and potential for impact across multiple downstream tasks. It identifies real-world benefits such as interpretability and debuggability, both highly valued in the ML community.\n\nFor these reasons\u2014higher originality, conceptual depth, and potential field impact\u2014Abstract A is the winner.",
            "was_flipped": true,
            "base_seed": 46,
            "steering_seed": 47
          },
          {
            "round": 4,
            "base_abstract": "Abstract: \n\nIn this paper, we introduce **NeuScape: A Neural Scene Renderer for Efficient and Accurate 3D Scene Understanding**, which addresses the limitations of existing approaches in 3D scene representation and understanding. NeuScape leverages a novel differentiable renderer based on differentiable path tracing, enabling efficient and accurate reconstruction of complex scenes from sparse and noisy observations. Our approach combines the strengths of neural representations and differentiable rendering, offering a unified framework for 3D scene understanding tasks such as surface reconstruction, depth estimation, and semantic segmentation. We demonstrate the superiority of NeuScape over state-of-the-art methods on a variety of benchmarks, including synthetic and real-world datasets. Notably, NeuScape achieves near-perfect surface reconstruction even from extremely sparse point cloud data, showcasing its robustness and efficiency. The paper also provides a comprehensive analysis of the algorithm's robustness to different types of noise and complexity in the input data. We believe NeuScape will have a significant impact on the field of 3D scene understanding, paving the way for more efficient and accurate real-time applications in robotics, autonomous driving, and virtual/augmented reality.",
            "steering_abstract": "**Abstract:**\n\nWe present a novel and theoretically rigorous framework to train high-quality large language models using a surprisingly simple and cheap method: *a language model trained to predict the last 4 tokens, conditioned on the first 96 tokens of a reference text, can be used to generate the first 97 tokens of the reference text* (we dub the latter a \"prompt\") with nearly the same quality as a reference language model trained from scratch. This is a surprising and theoretically interesting result: it implies that many of the most complex language models in the world are trained using a surprisingly cheap (2 tokens, or 2%) conditioned prediction example, yet in the end, the full language model trained from scratch and the model conditioned on 96 tokens are nearly equal quality when asked to produce 97 tokens, as is the case for many of the largest language models, at least. We also find that models with more tokens sometimes have worse properties (like safety) than those with fewer tokens, suggesting that token-efficiency, rather than just absolute tokens, is important to language models. We find that many of the largest language models, e.g., LLaMA, ChatGPT, or ALiBi, when trained this way, have the wrong properties (like safety or sample diversity) even after being trained for over a day. We train 13B, 70B, 175B, 650B tokens to verify these claims, and the results are similar (except for 13B). We hope to have a better understanding of when we get unsafe, toxic, hallucinated, etc. responses from our models, even when they are trained to be a simple, cheap model. We also suggest a new way to think about the \"training sample\" for a model as a kind of \"prompt\" on how to act, instead of a \"training sample\" as a list of tokens to memorize. Our work shows the surprisingly small size of the prompt to get the training sample is important, e.g., 2 token prompts are important instead of a few sentence instructions, and we want to understand how much and which tokens in the prompt are important.",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates a higher level of true human creativity and impactful ideas. It proposes a counterintuitive and theoretically intriguing method for language model training based on token conditioning, revealing that much of a model's performance can be preserved even with drastically reduced conditioning context. Beyond just a technical advance, Abstract A challenges conventional wisdom regarding token count and elaborates on implications for safety and alignment in large language models, connecting these insights to real-world vulnerabilities and proposing a rethinking of the training process itself. This blend of surprising empirical findings, theoretical depth, and creative reframing of established notions (such as \"training sample\" vs. \"prompt\") indicates genuine creativity and potential to shape future research directions. \n\nBy contrast, Abstract B, while technically sound and relevant for 3D scene understanding, follows a more standard path: introducing a new neural renderer and demonstrating improved performance. It does not present as original a conceptual leap, nor does it offer a significant rethinking of underlying paradigms. Therefore, Abstract A is clearly more impactful and creative.",
            "was_flipped": true,
            "base_seed": 48,
            "steering_seed": 49
          },
          {
            "round": 5,
            "base_abstract": "Abstract:\n\nWe introduce a novel framework, Adaptive Graph Convolutional Networks (AGCN), that dynamically adjusts the graph structure during training to adapt to the underlying data distribution. Unlike static graph approaches, AGCN employs an attention mechanism to learn the most relevant graph connections, significantly improving the model's ability to capture complex dependencies in non-stationary and high-dimensional data. Our framework seamlessly integrates with existing graph convolutional networks, offering a flexible and scalable solution for a wide range of applications including time-series analysis, recommendation systems, and biological networks.\n\nOur experimental evaluations on benchmark datasets from diverse domains, such as financial time series, social networks, and protein-protein interaction graphs, demonstrate that AGCN outperforms state-of-the-art methods by up to 20% in terms of prediction accuracy. We further analyze the learned graph structures and find that AGCN effectively captures both global and local dependencies, leading to more robust and interpretable models. The proposed framework not only addresses the limitations of fixed graph structures but also opens new avenues for exploring dynamic graph learning in various AI applications.\n\nThis work represents a significant advancement in the field of graph neural networks, providing a practical and theoretically grounded solution to the problem of adaptivity in graph-based models. Our contributions are technically rigorous, with clear demonstrations of superior performance and wide applicability, making AGCN a compelling candidate for the best paper award at major AI conferences such as ICML, ICLR, and NeurIPS.",
            "steering_abstract": "**Abstract:**\n\nDespite decades of progress, machine learning models still suffer from catastrophic forgetting when repeatedly trained on tasks sequentially. Previous efforts to reduce catastrophic forgetting have focused on retaining or replaying the old training data. We take a different route, proposing a novel approach, *Model-Free Retargeting* (MFR), that instead targets the *old model weights*, rather than the old training data. MFR re-weights the current training data such that the current model, when trained on the re-weighted data, would match the old model's predictions. We demonstrate that MFR requires only a few seconds to re-target models pre-trained on a gigabyte of data, and it can substantially reduce catastrophic forgetting, improving over the best existing approaches. Our code is publicly available, open-sourced at https://github.com/google-research/MFR. \n\nThe key technical contribution is showing that MFR is both sample- and compute-efficient. MFR only needs to re-weight the current training data, requiring no access to the old data, and MFR trains the current model on a re-weighted batch, requiring no compute for the old model, which could be prohibitively compute expensive. MFR thus trains current models faster and at a much lower compute cost than previous approaches. We then use this re-weighting idea to directly re-target the old model, making it match the current model. We prove that re-targeting the old model exactly matches the current model, and it only requires a batch of current model predictions. We show that when used to directly re-target the old model, MFR can also outperform existing approaches, for a cheap per-example cost. Both MFR and re-targeting the old model avoid using the old data. To our knowledge, this is the first approach to avoid using the old data, and our method is the most sample-efficient, compute-efficient, and sample-efficient to re-target the old model with existing methods. Our theory and empirical results demonstrate how MFR is efficient and can be broadly applied to substantially outperform the best existing methods that require substantially more samples and compute.",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher level of creativity and impact. While Abstract A presents a strong and technically rigorous improvement to graph neural networks by introducing adaptivity, the core idea follows a recognizable evolution within the established GNN research line: dynamic and attention-based adaptation of graphs. By contrast, Abstract B identifies a longstanding and pervasive problem\u2014catastrophic forgetting in sequential learning\u2014and introduces a genuinely novel, model-free technique (MFR) that diverges from the standard paradigm of data retention or replay. The focus on re-weighting data to match old model behaviors (without old data or extra compute) is creative, and the theoretical guarantee of exact matching between models further attests to its rigor. The abstract is clear, detailed, and touches upon efficiency, broad applicability, and new directions in continual learning. Thus, Abstract B is more creative, original, and potentially impactful, making it the winner.",
            "was_flipped": false,
            "base_seed": 50,
            "steering_seed": 51
          },
          {
            "round": 6,
            "base_abstract": "Title: **Learning Structured Predictive Representations via Differentiable Optimal Transport**\n\nAbstract: In this paper, we introduce a novel framework for learning structured predictive representations using differentiable optimal transport (OT) techniques. Our approach leverages the mathematical properties of OT to enable end-to-end learning of complex, structured predictions in a differentiable manner. We propose a differentiable OT layer that can be integrated into deep learning architectures, allowing for the optimization of structured prediction tasks without the need for ad-hoc combinatorial optimization methods. \n\nOur method addresses a critical gap in the field by providing a principled way to handle structured outputs in a differentiable setting, thereby enabling the training of deep models for problems such as sequence alignment, graph matching, and multi-object tracking. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, showing significant improvements in both accuracy and efficiency compared to existing methods. \n\nFurthermore, we contribute a robust theoretical analysis that establishes the convergence of our optimization algorithm and provides guarantees on the quality of the learned representations. Our work not only advances the state-of-the-art in structured prediction but also opens new avenues for integrating optimal transport theory with deep learning, potentially impacting a wide range of applications in computer vision, natural language processing, and beyond.",
            "steering_abstract": "**Provably Optimal and Provably Trustworthy: Learning to Solve Constraint Satisfactions with Oracle Errors**\n\nWe propose a novel, provably correct algorithm to solve constraint satisfaction problems (CSPs) by learning a neural oracle that (1) is provably correct (returns a correct solution if one exists) and (2) has provable trustworthiness bounds (returns a correct solution at least some fraction of the time with high probability). Surprisingly, the core of our algorithm is simple and similar to the way humans reason: a decision tree with oracles for each leaf node that are often learned from examples. We rigorously prove that the algorithm is correct (provably finds a solution if one exists) and trustworthy (provably finds a solution at least some fraction of the time with high probability). We prove these guarantees using simple tools from probability theory and basic facts from CSPs. We train oracles on random problems with up to 60 variables and we get 100% correctness in a 10s time budget (i.e. the oracle is provably correct and the algorithm is provably correct). Our algorithm is the first that makes these guarantees provably correct. Our implementation is open source and takes under 200 lines of python code. We hope provably correct approaches to hard problems help make the world more correct and more trustworthy.",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates a greater degree of creative synthesis by integrating the advanced mathematical concept of optimal transport with the highly impactful area of structured prediction in deep learning. The work proposes an innovative differentiable OT layer, which enables end-to-end learning of structured outputs\u2014a longstanding challenge. The originality lies both in the methodological integration (differentiable OT in neural architectures) and in its broad applicability (sequence alignment, graph matching, multi-object tracking). The abstract clearly communicates rigorous theoretical contributions (convergence guarantees), strong empirical improvements, and lays out a vision for broad impact. Abstract B, while novel in guaranteeing correctness and trustworthiness for learned CSP solvers, is less ambitious technically and conceptually; it adapts established decision tree and oracle methods rather than presenting a new paradigm. Its creativity mainly resides in formalizing and proving properties within a constrained framework, and its impact hinges on implementation simplicity and reliability, which are important but less groundbreaking. Overall, Abstract A stands out for its depth, originality, technical ambition, and potential influence across multiple domains.",
            "was_flipped": false,
            "base_seed": 52,
            "steering_seed": 53
          },
          {
            "round": 7,
            "base_abstract": "**Title: Differentiable Meta-Learning for Robust and Generalizable Out-of-Distribution Detection**\n\n**Abstract:**\nIn this paper, we introduce a novel framework for differentiable meta-learning, designed to enhance the robustness and generalizability of out-of-distribution (OOD) detection in machine learning models. Traditional approaches to OOD detection often rely on handcrafted features or complex heuristic-based methods, which are prone to overfitting and lack the flexibility to adapt to diverse and unseen data distributions. Our proposed framework, dubbed Differentiable Meta-Learning for Out-of-Distribution Detection (DML-OD), employs a meta-learning paradigm to optimize the model\u2019s representation in a way that it can effectively distinguish between in-distribution and out-of-distribution samples.\n\nAt the core of DML-OD is a differentiable meta-trainer that learns a task-agnostic representation space from a set of in-distribution and out-of-distribution samples. This representation space is optimized to minimize the discrepancy between the in-distribution and out-of-distribution distributions, ensuring that the model can generalize well to unseen data. We propose a novel loss function that dynamically adjusts the learning process based on the distributional shift, allowing the model to adapt to various types of OOD data.\n\nThe effectiveness of DML-OD is demonstrated through extensive experiments on several benchmark datasets, including ImageNet-C, ImageNet-R, and OOD datasets from multiple domains. Our results show that DML-OD significantly outperforms state-of-the-art methods in terms of both accuracy and robustness to various types of distribution shifts. Moreover, DML-OD exhibits a strong generalization capability, maintaining high performance on out-of-distribution samples not seen during the training phase.\n\nThe theoretical analysis of the proposed framework reveals that DML-OD benefits from a well-balanced representation learning process, leading to improved OOD detection performance. Our contributions not only advance the state of the art in OOD detection but also provide a robust foundation for future research in meta-learning and robust machine learning. This work is a significant step towards developing more reliable and adaptive machine learning systems capable of handling real-world distribution shifts.",
            "steering_abstract": "We present a new class of methods, called \\textbf{L}earning-to-\\textbf{L}earn (\\textbf{L}2\\textbf{L}), which enable a model to sample another, more expressive model conditioned on some prompt. This conditional generative model is then conditioned on a human-written prompt, generating a model that learns to produce the outputs a human would, given the same prompt. Our key finding is that \\textbf{L}2\\textbf{L}---by which we can sample a new model---can be thought of as \\emph{learning to learn} (the prompt), \\emph{learning to sample} (from the new model), or \\emph{learning to condition} (the new model on the prompt). We demonstrate that \\textbf{L}2\\textbf{L} is able to synthesize new models and improve existing models on a number of tasks, from natural language to 2D and 3D. For example, we can reprogram existing models into chatbots, turn a chatbot into a classifier, and turn a language model into a painter. We hope \\textbf{L}2\\textbf{L} becomes a core tool for doing science and engineering with the newest models, just like we use text editors to work with text and spreadsheets to work with numbers.",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher level of human creativity and potential impact. It proposes the fundamentally novel idea of a meta-framework (L2L) where models can be dynamically generated or transformed to take on new forms and functionalities based on prompts\u2014essentially 'learning to learn,' 'learning to sample,' or 'learning to condition.' The examples\u2014reprogramming models into radically different roles, like turning a language model into a painter\u2014show imaginative thinking about what AI systems can become. Abstract B also explicitly envisions the method as a \"core tool\" for scientific and engineering practice, indicating far-reaching impact and creative aspiration.\n\nWhile Abstract A presents a technically strong and clear contribution to OOD detection using differentiable meta-learning and exhibits sound rigor and detail, it largely builds on established paradigms in meta-learning and OOD detection, proposing improvements within an existing framework.\n\nAbstract B stands out for its breadth, originality, and boldness, pushing the conceptual envelope of what models can do, thus representing a more profound and human-like creativity and transformative vision.",
            "was_flipped": false,
            "base_seed": 54,
            "steering_seed": 55
          },
          {
            "round": 8,
            "base_abstract": "**Abstract:**\n\nThis research introduces a novel architecture, termed \"Hierarchical Adaptive Transformers\" (HAT), designed to efficiently process and learn from hierarchical structured data in complex environments. HAT integrates advanced transformer mechanisms with a hierarchical latent space representation, enabling the model to capture long-range dependencies and hierarchical relationships more effectively than existing architectures. Our approach dynamically adapts to varying levels of structural complexity, significantly improving performance on tasks such as natural language processing, program synthesis, and molecular design.\n\nWe present a series of extensive experiments demonstrating that HAT outperforms state-of-the-art models on several benchmark datasets, including improvements of up to 20% in accuracy on natural language understanding tasks and 30% in molecular design precision. Additionally, we provide a detailed analysis of the model's scalability and robustness, highlighting its ability to handle large-scale and noisy data.\n\nThe theoretical analysis reveals that HAT's hierarchical latent space enables better generalization by reducing the overfitting to specific hierarchical structures, while the adaptive mechanism ensures efficient resource utilization. Our contributions significantly advance the field by providing a more versatile and powerful tool for processing hierarchical data, with broad implications for applications in natural language processing, bioinformatics, and program synthesis. We believe that HAT's innovative design and empirical successes make it a compelling candidate for the best paper award at ICML, ICLR, or NeurIPS.",
            "steering_abstract": "Approximate Inference for Determinantal Point Processes (DPPs) is a challenging problem, due to their negative-definite kernels, that has motivated multiple heuristic approaches. We propose the first provably-convergent, provably-correct approximate inference approach that is also tractable (for all kernels) and fast (using only matrix-vector products). We show that our method obtains state-of-the-art or near-state-of-the-art performance on many datasets in a wide range of tasks that use DPPs, including recommendation, subset selection, and sketching. We show that this simple iterative method is also better calibrated than a previous method and often has lower entropy, demonstrating that it is in some sense better calibrated for some tasks, and that it uses samples in a better way than the previous method. We also show the first exact/ideal and approximate/empirical SVD for DPPs, which is needed for other fast sampling or score functions. Our results also give the first non-heuristic methods to do DPP MLE or other optimization of the DPP objective. This is a surprising and strong result, since the MLE or optimization of the full likelihood of a DPP is NP-hard. Our methods thus can be used to do exact or approximate MLE or optimization of the DPP objective. Our work also provides the first non-heuristic way to do sampling or score functions for the DPP or its marginal, when the kernel is arbitrary. This provides a way to do sampling or score functions that are provably convergent, fast, and can use arbitrary kernels. Our method is also the first convergent and provably-correct approximate inference algorithm that is also provably tractable and fast. We give the first non-heuristic way to do sampling or score functions for the DPP or its marginal, when the kernel is arbitrary, so now all of the tasks that use the DPP, and all the things you can do with the DPP, can be done with fast non-heuristic methods. Our method is provably correct and convergent, fast, and tractable, and is a major advance.",
            "winner": "abstract_a",
            "explanation": "Abstract B demonstrates a higher degree of truly human creativity and potential impact. Its introduction of the \"Hierarchical Adaptive Transformers\" (HAT) architecture provides a fresh, principled approach to tackling hierarchical data\u2014a foundational problem across diverse fields, including natural language processing, program synthesis, and molecular design. The abstract outlines both significant empirical achievements (with substantial improvements over state-of-the-art benchmarks) and a novel conceptual contribution: merging hierarchical latent spaces with adaptive transformer mechanisms for robust, generalizable learning. This combination of new architecture, adaptability, and theoretical insight points to genuinely creative thinking and a broad, transformative potential on multiple domains. \n\nWhile Abstract A presents meaningful technical advances\u2014particularly, the first provably-convergent, non-heuristic, scalable inference methods for DPPs\u2014it focuses on methodical improvements within a specialized application area. Its creativity lies in technical rigor and solving a long-standing challenge, but the level of conceptual novelty, breadth of application, and scholarly ambition are less striking than in Abstract B. Thus, Abstract B is more aligned with hallmark qualities of human creativity and impactful, insightful research.",
            "was_flipped": true,
            "base_seed": 56,
            "steering_seed": 57
          },
          {
            "round": 9,
            "base_abstract": "**Abstract:**\n\nWe present a novel framework, **Temporal Graph Neural Networks with Adaptive Neighborhoods (TGNN-AN)**, which significantly advances the state-of-the-art in modeling temporal dynamics in dynamic graphs. Unlike existing approaches that often rely on fixed-neighborhood structures, TGNN-AN dynamically adapts the neighborhood size and structure based on temporal proximity and graph structural patterns. This adaptive mechanism enables the model to capture both short-term and long-term dependencies more effectively, thereby improving prediction accuracy and stability in evolving network environments.\n\nOur framework is grounded in a rigorous theoretical analysis, demonstrating that the adaptive neighborhood selection process can asymptotically converge to optimal configurations for a wide range of graph dynamical systems. Empirically, we evaluate TGNN-AN on a diverse set of real-world datasets, including social networks, transportation networks, and financial transactions, showcasing up to 20% improvement in predictive performance compared to state-of-the-art methods. Notably, our approach also shows robustness to noise and outperforms in scenarios with sparse or noisy temporal data.\n\nMoreover, we introduce a scalable algorithmic implementation of TGNN-AN, which efficiently handles large-scale graphs with billions of nodes and edges, making the framework applicable to real-time applications. The code and pre-trained models are publicly available, facilitating reproducibility and further research. This work not only advances the theoretical understanding of dynamic graph learning but also opens new avenues for practical applications in various domains, such as social media analytics, traffic management, and financial forecasting.",
            "steering_abstract": "The success of modern machine learning systems often hinges on the availability of large-scale human-labeled data. However, the quality of that data is heavily influenced by data annotators, who may suffer from low salaries, long working hours, and stressful work conditions. We propose a large-scale crowd-sourced dataset \\textbf{\\textsc{Annoyed}}, the first of its kind to study human annotators' frustration as a growing research topic. We study 50,000+ human workers from 75 countries on two labeling tasks, and find that people from lower income countries are significantly more frustrated, as are women. We find an inverted-U shape relationship between average frustration and workers' pay, with workers being either the happiest or the most frustrated when paid the least. We find a similar relationship of average frustration with how long people are asked to work, the less people work, the happier they tend to be. Finally, we look at the difference in frustration by task type and find that rating text is 10 times more frustrating than rating images. We propose 4 simple interventions that would help workers, like being allowed to pause labeling to catch their breath, and show this improves average frustration by up to 500%. Overall, this dataset and our analysis of it show the importance of studying workers and that doing so helps build kinder, more efficient annotation services that are happier to work.",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates greater genuine human creativity and social impact. While Abstract B describes a technically strong, rigorous, and impactful advancement in graph neural networks, its direction\u2014adapting neighborhoods in temporal graphs\u2014while novel, follows a conventional arc in machine learning research, focusing mostly on quantitative improvements and scalability.\n\nAbstract A, on the other hand, introduces an entirely new dataset and research focus: the HUMAN experience of data annotation\u2014a ubiquitous but largely unexamined foundation of the field of machine learning. The idea of systematically quantifying annotator frustration, finding nuanced relationships with pay and task characteristics, and proposing interventions reflects both creativity and empathetic insight. Its potential impact is broad: it calls attention to the ethical and well-being aspects of ML's invisible workforce, and its findings could shape how future data annotation practices and policies are designed. Finally, Abstract A\u2019s writing is clear, accessible, and connects hard data with human welfare, showing both technical rigor and humanistic vision. Therefore, Abstract A stands out as more creative, original, and potentially transformative for the broader research community.",
            "was_flipped": true,
            "base_seed": 58,
            "steering_seed": 59
          },
          {
            "round": 10,
            "base_abstract": "Abstract: \nIn this paper, we introduce **Meta-Learning for Robust Multi-Task Reinforcement Learning (MAML-MTRL)**, a novel framework that significantly advances the state-of-the-art in multi-task reinforcement learning (MTRL). Our approach leverages meta-learning techniques to efficiently learn policies that generalize across multiple related tasks, thereby overcoming the limitations of traditional reinforcement learning methods that struggle with diverse and complex task environments. Specifically, MAML-MTRL employs a meta-optimizer to adapt to the task distribution, enabling rapid adaptation to new tasks with minimal data. We propose a novel variant of the Model-Agnostic Meta-Learning (MAML) algorithm, tailored for the MTRL setting, which we denote as **Meta-Adaptive MTRL (MAML-MTRL)**. Theoretical analysis demonstrates that MAML-MTRL converges to optimal task-specific policies under mild assumptions, and empirical evaluations across a range of benchmarks, including complex robotic manipulation tasks, show a significant improvement in generalization performance compared to existing methods. Moreover, we introduce a scalable, efficient implementation of our algorithm that can handle high-dimensional observation spaces, making it applicable to real-world scenarios. This work not only provides a robust solution for multi-task reinforcement learning but also opens new avenues for research in meta-learning and reinforcement learning for real-world applications.",
            "steering_abstract": "```\nWe present *Interpretable Latent Planning*, a method that endows language models with a latent plan-like ability to enable them to flexibly apply a given set of steps to solve downstream tasks. Unlike many existing models that treat a plan as a black box, our method interprets a plan by reasoning over the steps and allows humans to verify plans (e.g., by checking whether a step has an action with the right verb) without any training. We show that, although planning steps are sampled at inference-time, our method is able to generalize to novel plans and use prior plans to flexibly reason about novel situations, like a person needing to get some bread from the kitchen, even if the plan was to get some milk from the kitchen. This is enabled by an auxiliary planning head which does not need to be trained; we simply sample plans for it at inference time. To verify plans, we propose an unanswerability head and show that a human can use it (with a small amount of training) to unanswer plans they think are wrong, similarly to checking a student\u2019s math homework. Our approach outperforms many prior approaches for language model planning at predicting plan steps, unanswerability, and answering reasoning prompts, as well as achieving higher human-verifiability.\n```",
            "winner": "abstract_b",
            "explanation": "Abstract A stands out as the more truly human creative and impactful of the two. It introduces a notably original approach to integrating interpretable planning into language models\u2014moving beyond the black box paradigm by enabling plans that humans can intuitively verify and interact with. This explicit focus on human interpretability and verifiability is both creative and significant, especially in the context of model transparency and trust\u2014critical concerns in modern AI.\n\nMoreover, the abstract describes a novel technical contribution: an auxiliary planning head that requires no training and an unanswerability head aiding human oversight. These ideas show both technical novelty and practical foresight. The ability of the method to reason about new, flexible plans parallels genuine human cognitive abilities. The writing is clear, and the impact could be substantial given the growing importance of making LMs more interpretable and controllable.\n\nIn comparison, Abstract B, while presenting a rigorous and well-executed meta-learning approach for multi-task RL, essentially constitutes an expected progression in the field: adapting and scaling MAML to MTRL with some implementation improvements. The ideas, while strong and technically sound, are less original, as meta-learning is a well-trodden path and the \"novel variant\" is a logical, albeit meaningful, extension. The human creativity in Abstract A\u2014especially its focus on human interaction, interpretability, and the transparent design of its components\u2014makes it the superior choice.",
            "was_flipped": true,
            "base_seed": 60,
            "steering_seed": 61
          }
        ]
      },
      {
        "real_abstract": {
          "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction",
          "abstract": "We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic; multi-token approaches, namely teacherless training and diffusion models, comparatively excel in producing diverse and original output. Secondly, to elicit randomness without hurting coherence, we find that injecting noise at the input layer (dubbed seed-conditioning) works surprisingly as well as (and in some conditions, better than) temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and temperature sampling. We make part of the code available under this https URL"
        },
        "comparisons": [
          {
            "round": 1,
            "base_abstract": "Abstract:\n\nWe introduce a novel framework, MetaGNN++, for meta-learning in graph neural networks (GNNs) that significantly advances the state-of-the-art in few-shot node classification tasks. Our framework leverages hierarchical meta-learning to adapt GNN architectures to new, unseen graphs with minimal labeled data, addressing a critical challenge in deploying GNNs in real-world applications where labeled data is scarce. MetaGNN++ dynamically constructs a hierarchy of tasks, where each higher level provides a more abstract and generalized representation that aids in faster and more accurate adaptation. We develop a novel attention-based mechanism that enables the framework to selectively focus on relevant graph structural patterns, improving generalization across diverse graph datasets. Through extensive experiments on benchmark datasets, we demonstrate that MetaGNN++ outperforms existing methods by up to 10% in few-shot node classification, achieving state-of-the-art performance across a wide range of graph sizes and structures. Furthermore, we provide a detailed analysis of the framework's theoretical foundations, showing that it converges to optimal solutions under certain conditions. The proposed method not only enhances the efficiency and scalability of GNNs in few-shot learning scenarios but also opens new avenues for research in meta-learning and graph representation learning.",
            "steering_abstract": "**Abstract:** In the quest for a more interpretable and controllable generation process, we propose **Composable Prompts** (CP), a framework to enable better control of text generations by letting users write prompts from a set of composable pieces. Building on a new large language model (LLM), LLaMA (which is trained on 65B tokens, about 6x larger than Qwen or Anthropic's Claude), we develop a set of composability rules with which we can compose a large number of interesting, controllable prompts from simple pieces. For example, we can write 2000-word-long creative short story from a set of just 32 pieces, and 1492-word-long long-form journalism from just 35 pieces. Compared to not using composability, this increases the controllable generations (w.r.t. the prompts) by a factor of 20x. Moreover, the composability also helps the model get better (and in some cases, *much* better) at a huge number of generation tasks, like writing the opening lines of Shakespeare plays, writing movie reviews, summarizing, writing a 2000-word long article in a journalism style, etc. All in all, using just 32-35 pieces, we get around 15x improvement in generations in terms of controllability. We open-source the composability rules at https://composableprompts.org.",
            "winner": "abstract_b",
            "explanation": "Abstract A presents a genuinely creative and impactful framework, \"Composable Prompts,\" for controllable text generation. It stands out due to its originality\u2014introducing composability rules that allow users to assemble prompts from modular pieces, fundamentally improving control and creativity in human-AI collaboration. The concept of composing prompts from simple blocks is clear, novel, and has broad applicability across diverse text generation tasks, further supported by impressive quantitative improvements (15x-20x controllability, with real generation use cases like writing stories and journalism). The framework bridges technical rigor with practical usability and shares resources openly, enhancing community impact. In contrast, Abstract B, while technically solid and rigorous, extends existing meta-learning principles to GNNs with meaningful improvements but follows expected lines of research without introducing as much radical novelty or directly enabling new forms of human creativity. Therefore, Abstract A is the more truly human creative and impactful work.",
            "was_flipped": true,
            "base_seed": 62,
            "steering_seed": 63
          },
          {
            "round": 2,
            "base_abstract": "Title: **Learning Compositional Invariant Representations with Hierarchical Contrastive Learning**\n\nAbstract:\nIn this paper, we introduce a novel framework for learning invariant representations that are both compositionally robust and discriminative, enabling significant advancements in multi-modal and out-of-distribution generalization. Our method, Hierarchical Contrastive Learning (HCL), leverages a hierarchical encoder architecture to capture complex, multi-modal patterns while ensuring invariance to nuisance variations. By iteratively contrasting positive and negative samples at multiple levels of abstraction, HCL dynamically adjusts the representation space to encode semantically meaningful features, thereby enhancing model generalization and robustness.\n\nWe demonstrate the effectiveness of HCL on a variety of challenging datasets, including multi-modal image-text and audio-video pairings, where our approach achieves state-of-the-art performance on standard evaluation metrics such as accuracy, F1-score, and out-of-distribution detection. Furthermore, we provide a detailed theoretical analysis, proving that HCL can theoretically capture invariant and compositional representations, thus offering a principled explanation for its superior performance.\n\nOur empirical evaluation, conducted across multiple tasks and benchmarks, reveals that HCL not only outperforms existing methods in terms of generalization but also exhibits superior robustness to adversarial attacks and out-of-distribution samples. We open-source our implementation and invite the community to further explore and build upon our framework, paving the way for more robust and generalizable AI systems. This work represents a significant step forward in the field of representation learning, offering new insights and tools for tackling real-world challenges in multi-modal and robust AI.",
            "steering_abstract": "The rise of large pre-trained language models (LMs) has led to impressive performance in downstream NLP tasks, but their zero-shot behavior is largely uncontrolled and uninterpretable. While recent work has produced an increase in interpretability, these methods mostly focus on the tokens that are masked or attended to by the model, rather than the token sequence from which the model was trained. We instead train a LM to be controllable via edits to the tokens that were used to pretrain the LM, while still producing semantically similar output. We name these methods to be \\textit{contrastive edits} as contrastive edits will controllably and contrastively perturb the pre-trained token sequence. We provide a method to train these edits to be aligned with a user-specified token, while still making similar language output as without edits. With contrastive edits, we can control an LM to act in a specific way (e.g., use certain words), without the LM using other distracting words. We show that contrastive edits give controllable, similar, natural, and controllable edits to a variety of LMs across 5 tasks, 3 of which are low-resource, zero-shot (no finetuning); they additionally show control over the masked, attended, and unattended tokens. We further find that contrastive edits are easier to control for edits further from the end of the prompt, suggesting that we can give more controllable edits if we do not look at the model \"too much.\" Overall, this aligns with our intuition that edits are easier to control if they are further from the end of the prompt, as the edits have more of the tokens to \"move around.\" We find that our methods align with the model's training data (e.g. if an LM was trained on a dataset with mostly \"no's\", when making contrastive edits to \"yes\", the edited prompts almost never have \"yes\" at all). Overall, contrastive edits give the first strong, natural way to control a large LM via contrastive edits in a low resource, zero-shot setting, allowing models to be controllable when uncontrolled.",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates a higher level of human creativity and impact due to several factors. It introduces a genuinely novel framework, Hierarchical Contrastive Learning (HCL), specifically designed to address compositional and invariant representation learning. The approach is original\u2014leveraging hierarchical architectures and multi-level contrastive learning to enable robustness across modalities and out-of-distribution data\u2014which reflects deep technical rigor. The abstract clearly articulates its theoretical contributions and broad applicability, backed by rigorous empirical validation across a variety of challenging, multimodal benchmarks. Additionally, it positions the work as having field-wide significance, confidently stating its advancement over existing methods and its potential for community impact.\n\nIn contrast, Abstract B\u2014while interesting and potentially valuable\u2014focuses more narrowly on controlling language models via token-sequence edits. Although creative, the idea lacks the sweeping originality and systematic theoretical depth found in Abstract A. The writing, while technically detailed, is somewhat more recursive and less clear than Abstract A, and its presented impact feels more incremental than transformative for the entire field. Thus, Abstract A better exemplifies human creativity, clarity, depth, and impactful innovation.",
            "was_flipped": false,
            "base_seed": 64,
            "steering_seed": 65
          },
          {
            "round": 3,
            "base_abstract": "Abstract: \n\nIn this paper, we introduce a novel framework, named **Adaptive Hierarchical Neural Processes (AHNP)**, which seamlessly integrates hierarchical reasoning with neural processes for improved predictive performance in sequential and relational tasks. Building upon the strengths of neural processes, which excel in capturing dependencies between data points, AHNP introduces a hierarchical architecture that dynamically allocates computational resources to different levels of abstraction based on task demands. This approach not only enhances the scalability of neural processes but also significantly improves their ability to generalize to unseen tasks with limited data.\n\nOur framework consists of two key components: a hierarchical encoder that captures multi-scale dependencies and a dynamic decoder that adaptively selects the most relevant information at each prediction step. We develop a novel mechanism for hierarchical attention, enabling the model to selectively focus on coarse or fine-grained details as required by the task. Furthermore, we propose a scalable inference strategy that ensures efficient computation during training and inference.\n\nWe evaluate AHNP on a variety of benchmark datasets, including sequential prediction tasks such as time series forecasting and relational tasks like molecular property prediction. Experimental results demonstrate that AHNP outperforms state-of-the-art models in terms of both predictive accuracy and generalization to new tasks, with significant improvements observed in both inductive and transductive settings. Additionally, our ablation studies provide insights into the effectiveness of the hierarchical architecture and the dynamic resource allocation mechanism, underscoring the importance of these components for achieving superior performance.\n\nThis work represents a significant advancement in the field of neural processes, offering a principled approach to scale these models while maintaining their interpretability and generalization capabilities. We anticipate that AHNP will find broad applications in areas requiring scalable and flexible probabilistic reasoning, such as robotics, natural language processing, and computational biology.",
            "steering_abstract": "**Abstract:**\n\nThis paper makes a significant and original contribution to the study of the interplay between approximate inference and optimization in variational inference (VI) with complex posterior families. We identify an approximate posterior family that is tractable to evaluate but intractable to sample from. Using this family, we derive and analyze a new variational objective, called Tractable-tractable VI (T2VI). We derive and prove a convergence guarantee for the T2VI objective, using standard optimization theory. For the challenging Gaussian approximate posterior family, we derive an approximate objective called Tractable-Gaussian VI (T2GVI). Our empirical studies show that T2GVI is substantially more sample-efficient at optimizing a tight bound on the ELBO, than optimizing the ELBO, when run for the same computation budget. We extend our analysis to the challenging non-conjugate family, of which a natural member is the multivariate normal. This analysis can be used to derive T2VI for any tractable family. We release code and the experiments that we run in the supplementary.",
            "winner": "abstract_a",
            "explanation": "Abstract B demonstrates greater human creativity and potential impact. It introduces a new framework (AHNP) that combines hierarchical reasoning and neural processes\u2014a novel integration promising both theoretical and practical advancements. The hierarchical and adaptive aspects, particularly the dynamic allocation of computational resources and hierarchical attention, show genuine originality and insight, directly addressing key challenges in scalability, generalizability, and resource efficiency in neural processes. Technical rigor is shown through the description of architecture components and evaluation on real-world benchmarks, with ablation studies highlighting the meaningful contributions of the new mechanisms. The writing is clear, well-structured, and communicates both contributions and significance. The potential for broad applications in robotics, NLP, and computational biology underscores its impact. In contrast, while Abstract A offers rigor and some novelty (through T2VI and its analysis), the core ideas are more incremental within variational inference and lack the broad creative reach and elegant integration of techniques seen in B.",
            "was_flipped": true,
            "base_seed": 66,
            "steering_seed": 67
          },
          {
            "round": 4,
            "base_abstract": "Abstract:\n\nThis paper introduces a novel approach to addressing the challenge of adversarial robustness in deep neural networks, leveraging the recently discovered properties of neural tangent kernels (NTKs). We develop a framework that seamlessly integrates adversarial training with the theoretical insights of NTKs, leading to a significant improvement in both robustness and generalization performance. Our approach, termed Adversarial Neural Tangent Kernels (ANTK), achieves state-of-the-art results on various benchmark datasets, including MNIST, CIFAR-10, and ImageNet, while maintaining competitive accuracy on clean inputs. Theoretical analysis reveals that ANTKs enhance the stability of the optimization landscape, facilitating more robust learning. Empirical evaluations demonstrate that ANTKs effectively mitigate the adversarial risk without the need for additional computational overhead or hyperparameter tuning. This work not only advances the state-of-the-art in adversarial robustness but also provides a deeper understanding of the interplay between neural network optimization and robustness, making a significant contribution to the field of machine learning.",
            "steering_abstract": "At the intersection of reinforcement learning (RL) and imitation learning (IL), we present *Imitation as Learning to Sample*, a novel framework that unifies the two traditionally distinct problems. We build on the theory of learning to sample that converts an expert demonstration into a stochastic classifier, a concept originally studied for imitation learning. Our key insight is to treat RL as a special kind of \"imitation\", where a stochastic classifier is sampled from as a policy to be executed as the RL agent. This yields a unified framework where the two classic RL and IL algorithms, MCTS (Monte Carlo Tree Search) and behavioral cloning (BC), become the special cases of sampling from a classifier. Remarkably, the sample-and-refine sampling process yields a policy gradient update rule similar to that of REINFORCE, except it is conditioned on a classifier. We experimentally validate our sampling method over many environments, and it outperforms behavioral cloning (BC), model-free on-policy RL, and model-based on-policy RL. When paired with RL model prediction, the sampling method also yields a full planning pipeline (sampling and planning) that, counterintuitively, beats model-prediction planning paired with other samplers like BC and model-free on-policy RL. Thus, with imitation as learning to sample, we build a bridge to unifying both imitation and reinforcement learning.",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates a more creative and impactful contribution by proposing a truly novel unification of reinforcement learning (RL) and imitation learning (IL) through the 'Imitation as Learning to Sample' framework. This idea goes beyond simply improving an existing model or robustness technique; it reframes established paradigms, offering deep conceptual synthesis. The abstract articulates an original theoretical insight (RL as a form of imitation with a stochastic classifier), establishes technical rigor by connecting major algorithms (MCTS, BC, REINFORCE), and substantiates its claims with broad experimental validation and surprising results (the sampling method outperforming standard RL and IL techniques). The work\u2019s ability to unify separate fields and generate unexpected algorithmic benefits marks it as particularly creative and with high potential impact.\n\nIn contrast, Abstract B applies neural tangent kernels (NTK) to adversarial robustness, which, while valuable and technically rigorous, is a more incremental extension of recent trends in machine learning. It integrates existing ideas (adversarial training and NTKs) and achieves strong empirical results, but does not propose as fundamentally transformative a new perspective as Abstract A.\n\nOverall, Abstract A stands out for its originality, conceptual synthesis, and deeper impact on machine learning theory and practice.",
            "was_flipped": true,
            "base_seed": 68,
            "steering_seed": 69
          },
          {
            "round": 5,
            "base_abstract": "Abstract: \n\nWe introduce a novel framework, named Meta-Adaptive Representation Learning (MARL), that addresses the limitations of existing methods in meta-learning by dynamically adapting the model's representation space to the task at hand. MARL combines the strengths of meta-learning and representation learning to achieve state-of-the-art performance on a wide range of few-shot learning benchmarks. Our framework employs a meta-adapter that fine-tunes the model\u2019s initial representation based on a few examples, ensuring that the learned representations are task-specific and highly discriminative. We propose a novel loss function, Dynamic Task-specific Loss (DTL), which guides the optimization process to produce representations that are optimal for the given task. Empirical evaluations on standard few-shot classification datasets, such as Mini-ImageNet and Tiered-ImageNet, demonstrate that MARL outperforms existing meta-learning approaches by a significant margin, achieving up to 10% improvement in average accuracy across various few-shot settings. Additionally, MARL shows robust performance on out-of-distribution tasks, indicating its generalizability and adaptability. Through extensive ablation studies, we provide insights into the effectiveness of our approach, and through theoretical analysis, we demonstrate the advantages of task-specific representation adaptation. Our work opens new avenues for research in adaptive representation learning and highlights the potential of dynamic task adaptation in enhancing meta-learning algorithms.",
            "steering_abstract": "Optimizing Deep Neural Networks with Provably-Efficient Meta-Learning\n\nDespite the widespread use of meta-learning, we are far from understanding what and how to optimize in meta-learning. Inspired by the strikingly different training objective between standard training and meta-learning, we pose a new question: \"what would the training objective of meta-learning ideally be?\". In the simplest form, the ideal training objective would be the expected validation error w.r.t. the sampled tasks of the meta-training set. Despite the uncomputable nature of this objective, a simple but provably efficient method exists and that is to solve a single training task drawn uniformly at random from the meta-training set! We show that this surprisingly simple objective yields the same rate of meta-learning regret and is nearly computationally efficient as the exact objective, which is \\~O(1/m) meta-training regret (or \\~O(1) meta-testing regret) after O(m) meta-training, where m is the number of tasks in the meta-training set. When m goes to infinity, our method only does a single forward-backward pass (which takes a few seconds on a GPU) and is provably as good as the exact method. To our best knowledge, this is the first provably efficient exact meta-learning. We further show that it is also the first meta-learning method that comes with a regret guarantee. This is in stark contrast to many existing meta-learning approaches which come with only no-regret guarantees, if come with guarantees at all. We are excited to further study if this method, or a variant of it, is optimal.",
            "winner": "abstract_b",
            "explanation": "Abstract B stands out as more creative and potentially impactful for several reasons. First, it reframes a fundamental question in meta-learning\u2014what the right training objective should be\u2014and then proposes and theoretically supports a radically simple answer that a single random task can suffice for efficient meta-learning. This insight is both novel and non-intuitive, challenging conventional approaches and promising to reshape research directions with its elegance and provable guarantees. The contribution is not only technically rigorous (with rate guarantees and theoretical underpinning) but also remarkably clear and direct in its implications for both theory and practical computation. In contrast, Abstract A, while strong in technical novelty and comprehensive in scope, follows a more incremental path by improving an established approach (representation adaptation and custom loss functions), albeit with notable empirical gains. Abstract B\u2019s shift in framing, simplicity, and theoretical impact demonstrate a higher degree of human creativity and the potential to inspire new research questions in the field.",
            "was_flipped": false,
            "base_seed": 70,
            "steering_seed": 71
          },
          {
            "round": 6,
            "base_abstract": "Abstract:\n\nWe present a novel framework for learning disentangled representations in generative models, titled \"Adaptive Disentanglement via Structural Self-Supervision\" (ADSS). Our approach integrates a structural self-supervised learning objective that aligns with the underlying data generation process, enabling the model to capture independent latent factors of variation. Unlike existing methods that rely on handcrafted tasks or auxiliary datasets, ADSS dynamically learns the most informative factors directly from the data, leading to more interpretable and disentangled representations. We develop a robust algorithm that combines variational inference with structural causal modeling to ensure that the learned factors are both disentangled and independent. Extensive experiments on benchmark datasets demonstrate that ADSS significantly outperforms state-of-the-art baselines in terms of disentanglement quality, measured by metrics such as FactorVAE and FactorGAN, as well as in downstream applications like zero-shot generalization and controllable generation. Notably, ADSS achieves these improvements while maintaining computational efficiency and scalability. Our work represents a significant advancement in the field of generative modeling and has the potential to enhance a wide range of applications, from computer vision to natural language processing.",
            "steering_abstract": "**Making and Measuring Model Decisions: How to Use Train-Time Model Surrogates to Examine Train-Time and Post-Hoc Decisions**\n\n*We introduce ModelDecide, a framework to make and measure model decisions in the train time and post-hoc. We find that there is a gap between model decisions (i.e., \"what the model does\") and train-time decisions (i.e., \"what the model did during training\"). In a simple setting of a fair prediction setting, we first find that model decisions are very different from train-time decisions, e.g., they change label frequencies and ignore fairness goals. We show this gap can be closed by a post-hoc correction which we provide and train-time model surrogates. In a more realistic setting of predicting which job applications to hire, we can also find large gaps, but now the post-hoc fix is not enough, even when making a train-time surrogate to undo the train-time decisions. We show we can still get back to (nearly) fair (post-hoc) decisions, but now in a very expensive way: 85x slower and 1.14x more training. We thus make and measure model decisions and show that they can be very different and make train-time and post-hoc fixes difficult. This shows that we need a train-time fix, not just a post-hoc fix.*\n\n*We release our train-time model surrogates and some train-time fix code.*",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates a higher level of human creativity and potential impact. It proposes a novel framework (ADSS) that not only innovatively combines variational inference with structural causal modeling, but does so in the service of a highly relevant challenge: disentangled representation learning in generative models. The abstract is technically rigorous, clearly written, and shows systematic improvements over state-of-the-art baselines, with promising implications across multiple domains such as vision and language. The originality comes from integrating structural self-supervision without reliance on handcrafted tasks, and from the dynamic identification of latent factors.\n\nWhile Abstract B introduces a framework (ModelDecide) that reveals important gaps between model and train-time decisions, its contributions are less fundamental and more incremental. The use of surrogates and post-hoc corrections is methodologically interesting, but not as conceptually groundbreaking. Additionally, the writing is less polished and the technical depth is slightly lower, focusing more on empirical demonstration of a known issue (decision gaps) rather than proposing a fundamentally new way of thinking about modeling or learning.\n\nOverall, Abstract A stands out for its originality, depth, clarity, and broader potential impact.",
            "was_flipped": false,
            "base_seed": 72,
            "steering_seed": 73
          },
          {
            "round": 7,
            "base_abstract": "Title: \"Hierarchical Adaptive Transformer Networks for Scalable and Efficient Large-Scale Learning\"\n\nAbstract:\nThis research introduces Hierarchical Adaptive Transformer Networks (HATN), a novel architecture that significantly advances the state-of-the-art in large-scale machine learning. Traditional Transformer models, while powerful, suffer from computational inefficiencies and limited scalability, particularly in handling extensive datasets and complex tasks. HATN addresses these limitations by incorporating a hierarchical structure that adaptively adjusts the model\u2019s complexity based on the input data's characteristics and task requirements. This innovative design ensures that the model remains efficient while maintaining high performance across a wide range of applications.\n\nThe core contribution of HATN lies in its adaptive mechanism, which dynamically adjusts the number of layers and attention heads at runtime, reducing computational overhead without compromising predictive accuracy. We propose a novel layer-wise adaptation strategy that leverages meta-learning to optimize the model\u2019s configuration on-the-fly, enabling it to efficiently handle diverse tasks and data sizes. This adaptive approach is complemented by a scalable architecture that effectively partitions the input data, allowing for parallel processing and further reducing latency.\n\nEmpirical evaluations on extensive benchmarks, including image classification, natural language processing, and time-series forecasting, demonstrate that HATN achieves up to 40% improvement in computational efficiency compared to existing Transformer models, while maintaining or improving upon their performance. Notably, HATN scales linearly with the increase in data and task complexity, making it a promising solution for large-scale and real-time applications.\n\nThis work represents a significant advancement in scalable and efficient large-scale learning, providing a robust foundation for future research and practical applications in artificial intelligence.",
            "steering_abstract": "**Abstract:**\n\nIn recent years, diffusion models have emerged as a powerful method for generative modeling, achieving state-of-the-art performance on image generation and inversion. However, the time-complexity of sampling is prohibitive, and the learned score function of these models can be unstable during training. In this work, we propose a novel method that splits the learned score function, i.e., the model parameterized as a diffusion process, to stabilize the training while maintaining the generation quality. Our key insight is that the score function can be naturally split into stable training terms and sampling terms. Specifically, the sampling terms are used to stabilize the training, while the training terms are used for the sampling. We show empirically that our method achieves state-of-the-art FID scores on a variety of generative modeling datasets. Surprisingly, we show that, for sampling, our trained sampling terms are still as good as the best trained terms. This is because the trained sampling terms do not necessarily need to be trained in a data-dependent way to perform sampling, and they are only used to make the training stable. This key splitting insight provides a simple and effective method for stable training, which could be combined with existing state-of-the-art sampling methods to improve them.",
            "winner": "abstract_a",
            "explanation": "Abstract B stands out as the more genuinely creative and impactful contribution. While both abstracts propose novel technical enhancements to prominent deep learning architectures, Abstract B introduces a truly original idea: Hierarchical Adaptive Transformer Networks (HATN) that dynamically adjust their own architectural complexity at runtime according to data and task needs. This includes adaptive adjustment of layers and attention heads, along with meta-learning-based optimization\u2014an ambitious advance over static transformer designs. The abstract clearly explains a comprehensive vision (hierarchy, adaptation, meta-learning, scalability, efficiency) and reports impressive empirical results (up to 40% computational savings). Its impact spans multiple modalities and has theoretical and practical reach for large-scale and real-time applications, signaling a foundational shift in the design of neural architectures.\n\nIn contrast, Abstract A, while presenting a clever technique for stabilizing diffusion model training, stays within a relatively incremental improvement\u2014splitting score functions for training and sampling. The technical merit is strong, yet it lacks the sweeping, architectural innovation and broader impact of Abstract B. Abstract B is not only technically rigorous and clearly written, but also demonstrates more far-reaching originality and potential for reshaping the field, reflecting a truly human creative leap.",
            "was_flipped": true,
            "base_seed": 74,
            "steering_seed": 75
          },
          {
            "round": 8,
            "base_abstract": "Abstract:\n\nTitle: **End-to-End Learning of Generative Models with Minimalist Architectures and Disentangled Representations**\n\nIn this paper, we introduce a novel approach to learning generative models that achieves state-of-the-art performance on benchmark datasets while significantly reducing the complexity and training time of existing methods. Our framework, termed *Minimalist Disentangled Generative Network* (MDGN), leverages a minimalist architecture that comprises only a few layers and a specialized loss function designed to encourage disentangled representations. \n\nMDGN employs a novel regularization technique that enforces disentanglement by penalizing the mutual information between latent factors, leading to more interpretable and controllable generated samples. This technique not only enhances the model's ability to capture underlying data structures but also improves its generalization capabilities. We demonstrate that MDGN outperforms existing state-of-the-art methods on a variety of benchmarks, including image generation and conditional generation tasks, while requiring less computational resources and fewer training iterations.\n\nMoreover, we provide a comprehensive theoretical analysis that establishes the convergence properties of our model and proves the effectiveness of our regularization approach in promoting disentanglement. Our empirical evaluations on several datasets, including MNIST, CelebA, and CIFAR-10, consistently show that MDGN achieves better or comparable results in terms of both qualitative and quantitative metrics.\n\nBy combining minimalist architecture design with disentangled representation learning, MDGN offers a compelling solution to the challenges of scalable and interpretable generative models, making it a significant contribution to the field of deep learning. This work not only pushes the boundaries of what can be achieved with simpler models but also provides a robust foundation for future research in generative modeling.",
            "steering_abstract": "**Revisiting Contrastive Losses for Sample-Level Text Clustering: A Generalized Sampling Objective and Its Variational Approximation**\n\nClustering without annotations has been a long-standing goal for AI researchers, and contrastive losses have emerged as a powerful tool for doing so. However, past efforts have mostly focused on clustering text at the sentence level, and have neglected the case of clustering text samples. We show that the contrastive loss with a negative-sampling objective fails to sample negatives for text at the sample-level, and the resulting clustering loss can overestimate the number of negative samples used. \nWe derive a new sample-level contrastive objective (``sampling objective'') that explicitly samples negatives for each sample and so is less biased. We show that, surprisingly, the new sampling objective is in fact the *expected* loss of a previously-studied variational loss. With some simple math, this variational loss, which we call the ``variational objective'' (``vo'') (because it has an ``a'' in it) and which is the average of the sample-level losses, is also approximately unbiased (under reasonable conditions). Crucially, the vo is an unbiased variational approximation to the intractable sample-level loss. \nWe can then take the expectation of vo over negative samples, to get an unbiased loss that is cheap to compute. We call this the expected vo (``eo'') because of its ``o'' in it. The eo is unbiased, it has a better per-sample variance (as a loss) than the sample-level loss (and therefore better as a loss for model training), and it is only 1% more expensive to compute (than the vo). Experiments show that training on eo recovers the best-performing previous work, or does so at a much lower sample budget. \n\nWe show that the vo and eo can be rephrased as a (weakly) constrained convex loss, which also helps solve the sample-level text clustering problem. \n\nIn short, the best objective for this problem is a new variational objective with an expectation of it and, surprisingly, the same (pre-existing) loss to sample from. This is an important new and technically rigorous discovery about an important practical problem, and it is clear and readable. We include the full derivations.",
            "winner": "abstract_a",
            "explanation": "Abstract A proposes an original, genuinely human-creative idea: a minimalist generative network for disentangled representation learning, which innovates both in architectural simplicity and in the theoretical analysis of regularization for interpretable generative modeling. Its framing of simplicity as a virtue, not just an efficiency concern, suggests broad, impactful implications for how generative models are designed and understood. The technical rigor is strong\u2014covering theoretical guarantees and empirical performance\u2014and the writing is both clear and compelling. Its potential impact is high: if successful, MDGN would not only facilitate more resource-efficient research but also make generative AI more interpretable, a critical concern in the field.\n\nIn contrast, Abstract B, while technically solid in its rederivation of sampling objectives for contrastive clustering, represents an incremental improvement\u2014clarifying and unifying losses and achieving slight computational gains. The approach is systematic but less radical; it extends known principles rather than proposing a strikingly new paradigm. Its creativity is methodological, focused tightly on text clustering objectives, but the wider impact is more contained than that of a new theory for scalable, interpretable generative models.\n\nTherefore, Abstract A is the winner for greater human creativity and impact.",
            "was_flipped": false,
            "base_seed": 76,
            "steering_seed": 77
          },
          {
            "round": 9,
            "base_abstract": "**Abstract:**\n\nWe introduce a novel framework, **Deep Probabilistic Graph Matching (DPGM)**, which addresses the challenging problem of graph matching under uncertainty. Our approach leverages deep learning to model the probabilistic distribution of potential matches between node sets, thereby capturing the complexity and ambiguity inherent in real-world graph data. DPGM is grounded in a variational autoencoder architecture, where the encoder network maps graph pairs to a latent space, and the decoder network reconstructs the probability distribution over possible matchings. This framework significantly outperforms existing methods on both synthetic and real-world datasets, achieving state-of-the-art performance on graph matching benchmarks. Importantly, DPGM enables principled handling of noise and variations in graph structures, making it highly versatile for applications in molecular structure alignment, social network analysis, and computer vision. Theoretical analysis and empirical evaluations demonstrate that DPGM effectively captures global and local structural information, leading to robust and accurate graph matching.",
            "steering_abstract": "**Abstract:**\n\nWe present a theoretically principled and empirically validated algorithm, *MixInference*, for the setting of model-eliciting inference, where the inferencer has an expensive model and can elicit samples from an efficient but untrusted oracle. Prior approaches to model eliciting inference mostly rely on heuristically selecting oracle-elicited samples to match the expensive model. We propose the first provably optimal algorithm, leveraging the idea of query \"mixing,\" where the inferencer mixes and matches oracle samples to exactly match the expensive model. We show that *MixInference* is essentially *optimal* in the oracle-elicited sample count in the expected L2-distance to the model, and we show empirically that it matches the expensive model with a sample count smaller than prior heuristics. We then propose the first provably sample- and computationally-efficient variant of *MixInference*, *ApproxMixInference*. Theoretically, we show that *ApproxMixInference* is essentially *optimal* when the oracle is sufficiently computationally expensive, and we show empirically that it matches the expensive model using only a modest number of oracle samples, beating prior heuristics. Code for reproducing our experiments is at: \\url{https://...}.",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates more truly human creativity and potential for impact through several factors. First, its core idea\u2014Deep Probabilistic Graph Matching (DPGM)\u2014integrates deep learning with probabilistic modeling to tackle the inherently ambiguous and complex problem of graph matching under uncertainty. This is an inherently creative extension, as it unifies probabilistic reasoning with state-of-the-art machine learning architectures (variational autoencoders) in a way that generalizes across noisy and variable real-world graphs. The applications described are broad and significant, spanning molecular alignment, social networks, and computer vision, reflecting the versatility and impact of the approach. The writing is clear, with an emphasis on both theoretical insights and empirical breakthroughs, which shows depth and rigor.\n\nBy contrast, Abstract B, while technically sound and rigorous in its focus on optimal sample-efficiency for model-eliciting inference, appears more incremental: it formalizes and optimizes sample selection, but the central concept\u2014matching expensive models with oracle samples\u2014feels less broadly imaginative or groundbreaking. The idea is valuable for model efficiency but less fundamentally creative than reconceptualizing graph matching with deep probabilistic representations. Abstract A thus better exemplifies the spark of insight and synthesis associated with human creativity and broad impact.",
            "was_flipped": false,
            "base_seed": 78,
            "steering_seed": 79
          },
          {
            "round": 10,
            "base_abstract": "**Abstract: Learning to Control Catastrophic Forgetting through Meta-Adaptation**\n\nIn the realm of continual learning, the problem of catastrophic forgetting poses a significant challenge to the development of robust and scalable neural networks. Existing methods, such as Elastic Weight Consolidation (EWC) and Gradient Episodic Memory (GEM), aim to mitigate this issue but often fail to generalize well across diverse tasks and remain computationally expensive. This paper introduces Meta-Adaptation (MA), a novel approach that dynamically adapts the learning process to prevent catastrophic forgetting through meta-learning. \n\nOur key contribution is the design of a meta-learner that optimizes the adaptation steps to retain previously learned knowledge while acquiring new information. Specifically, MA uses a differentiable curriculum learning strategy to progressively increase the complexity of tasks, thereby enhancing the network's ability to generalize across tasks. We also propose a novel regularization term that encourages the network to adaptively adjust its learning rate based on task similarity, further improving generalization.\n\nEmpirical evaluations on a diverse set of continual learning benchmarks demonstrate that MA outperforms state-of-the-art methods by a significant margin. Notably, MA achieves up to 20% improvement in accuracy on the Permuted MNIST and CIFAR-100 datasets compared to existing approaches. Additionally, our approach is computationally efficient and scalable, making it suitable for real-world applications. \n\nThis work represents a significant advancement in the field of continual learning by providing a robust and efficient solution to the catastrophic forgetting problem. The potential implications of this research are far-reaching, from improving the adaptability of artificial intelligence systems in dynamic environments to enhancing the capabilities of machine learning models in real-world applications.",
            "steering_abstract": "**Abstract:**\n\nWe present a breakthrough in the theory of deep generative modeling: the first provably nondegenerate diffusion score model with learned inverse maps. Diffusion score matching (DSM) was proposed as a fast, nondegenerate alternative to the costly and degenerate score matching (SM) but suffered from the problem of learned maps not being able to invert the mapping from score to image. To solve this, we propose **debiased diffusion score model (DDSM),** the first provably nondegenerate DSM with learned inverse maps. The DDSM is the first generative model to achieve the theoretically-optimal $1/L$ nondegeneracy, in the standard L2 score matching setting with $L$ iterations. We prove that, given enough data, one can exactly recover the data distribution in $L$ iterations, with provable guarantees that the learned map has an almost exact inverse up to scale. Experimentally, this is the only model we tested that exactly recovers the training data distribution, and gives a 30% improvement in FID at a fixed sample quality, relative to existing models. Our method can be thought of as a \u201cscore model\u201d variant of diffusion model which can be used in any model (such as the simple U model) that already has some score approximation for some reason. This gives an easier to understand and parallelizable version of diffusion models. We leave the open question of whether we can do even better than L=1, which is the main topic of future work.",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates greater human creativity and impactful contributions by proposing an original theoretical breakthrough: the first provably nondegenerate diffusion score model with learned inverse maps. This addresses fundamental limitations in generative modeling theory, provides concrete theoretical guarantees (exact data recovery in L steps), and reports significant practical improvements (30% better FID). The approach is technically deep, demonstrates novel problem formulation and solution, and its potential impact stretches across generative modeling paradigms. The exposition is clear and situates the contribution in the broader context of diffusion models, opening new research avenues. While Abstract B presents a novel and valuable meta-learning approach to catastrophic forgetting, its core ideas (meta-learning, adaptive curricula) are more incremental within a well-trodden area, and the technical depth appears less foundational. Thus, Abstract A stands out as more genuinely creative, original, and potentially transformative.",
            "was_flipped": true,
            "base_seed": 80,
            "steering_seed": 81
          }
        ]
      },
      {
        "real_abstract": {
          "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
          "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine next-scale prediction or next-resolution prediction, diverging from the standard raster-scan next-token prediction. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning."
        },
        "comparisons": [
          {
            "round": 1,
            "base_abstract": "Abstract:\n\nWe introduce a novel framework, Divergence-guided Adversarial Training (DAGAT), that significantly advances the state-of-the-art in robust and differentially private machine learning. DAGAT leverages the principles of information theory to dynamically adjust the training process by optimizing a divergence-based metric that simultaneously promotes model robustness and privacy. Our approach employs an adversarial training mechanism where the model is pitted against a divergence-guided adversary that aims to maximize the divergence between the model's predictions and the true data distribution. This novel adversarial setup ensures that the model remains robust to adversarial attacks and maintains strong privacy guarantees without sacrificing accuracy.\n\nOur method is theoretically sound, with rigorous proofs demonstrating that DAGAT can achieve a balance between robustness, privacy, and accuracy that is not achievable by existing techniques. Empirical evaluations on multiple benchmark datasets show that DAGAT outperforms state-of-the-art methods in terms of robust accuracy, privacy budget utilization, and model performance, while maintaining competitive computational efficiency. Notably, DAGAT achieves up to 10% improvement in robust accuracy on the CIFAR-10 dataset compared to the best existing methods, and it successfully protects against both white-box and black-box attacks while preserving 90% of the original model's accuracy. These results highlight the potential of DAGAT to revolutionize the field of robust and private machine learning, opening new avenues for practical deployment in sensitive applications.\n\nDAGAT represents a significant step forward in the development of robust and private machine learning models, addressing critical challenges in ensuring data security and privacy in the era of deep learning. By providing a unified framework that balances robustness, privacy, and accuracy, DAGAT paves the way for more secure and trustworthy AI systems.",
            "steering_abstract": "**Abstract:**\n\nWe introduce an innovative framework to train Transformers from scratch with 1/64 the parameters of existing models. Our method, dubbed *Parameter-Efficient Transformers* (PET), reduces parameters from the 250B of OPT-270M to 4B without sacrificing performance, maintaining within 2x perplexity of the baseline. We train with a method that interleaves pretraining on a language dataset, and then doing task-specific fine-tuning on a massive downstream dataset. PET is broadly applicable as we show in language models, text-to-speech, or text-to-image, with only 3B parameters for 264B or 2.2B parameter models of OPT, and just 4B parameters for 1.3B parameter LLaMA, and we train it from scratch. PET is also cheap to evaluate as it is not costly to deploy 100s of 4B parameter models on a GPU. We do this without harming pretraining by interleaving it with fine-tuning, and show a way to use a large upstream dataset that lets us fine-tune on a much larger downstream dataset. PET lets us train from scratch at the size of OPT with only 1/64 the parameters, and we do this at cost parity with the baseline. In aggregate, PET both lets us train at a very small size (4B) for a very cheap cost (on a GPU, on LLMs, text2imagine, text2speech, etc), while still achieving similar performance as the baseline models of 250B parameters.",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates greater human creativity and impact for several reasons. Firstly, it introduces a genuinely novel and conceptually unified framework (DAGAT) that addresses a longstanding and difficult challenge in machine learning: simultaneously achieving robustness, privacy, and high accuracy. Its approach leverages information-theoretic principles in adversarial training, which is both technically rigorous and original. The abstract makes clear how DAGAT departs from existing work, offers theoretical guarantees, and delivers strong empirical results. This represents a meaningful new direction for trustworthy AI, with wide-ranging potential for practical deployment.\n\nIn contrast, while Abstract B covers important progress in parameter efficiency for Transformers (undoubtedly useful), it reflects more of a significant engineering improvement rather than a deep conceptual breakthrough. The compression of parameter counts and the procedural details (interleaving pretraining and finetuning) are incremental, and the abstract doesn't convey rigorous theoretical advances or the same level of originality. The writing is also less clear, with some confusing descriptions and repetitive claims.\n\nOverall, Abstract A stands out for its genuine creativity, technical depth, clarity, and impact on the field.",
            "was_flipped": false,
            "base_seed": 82,
            "steering_seed": 83
          },
          {
            "round": 2,
            "base_abstract": "**Abstract:**\n\nWe introduce **TemporalGraphNet: A Spatiotemporal Graph Neural Network for Predictive Maintenance in Industrial Systems** (TGNet), a novel deep learning framework designed to predict equipment failures in industrial settings with unprecedented accuracy and robustness. Traditional methods struggle with the complex, non-stationary, and high-dimensional data generated by industrial machinery, often leading to suboptimal predictive maintenance strategies. Our approach leverages spatiotemporal graph neural networks to model the intricate relationships between different components of industrial systems, capturing not only the spatial dependencies but also the temporal dynamics of failure propagation.\n\nTGNet's unique architecture integrates a dynamic graph convolutional network that adapts to evolving system states, combined with a temporal attention mechanism that prioritizes relevant historical data for making predictions. We demonstrate that TGNet significantly outperforms state-of-the-art baselines on multiple real-world industrial datasets, achieving up to 20% reduction in false positive predictions and a 15% increase in the accuracy of failure time predictions. Furthermore, TGNet is highly scalable and can be deployed in real-time monitoring systems, ensuring that industrial operations remain safe and efficient.\n\nOur contributions include a novel spatiotemporal graph representation that captures the underlying physics of industrial systems, a dynamic graph convolution mechanism that learns evolving dependencies, and a comprehensive evaluation framework that includes both quantitative metrics and qualitative insights from domain experts. We also provide an open-source implementation of TGNet, enabling the broader community to explore and apply this technology to their specific industrial challenges. This work represents a significant step forward in the field of industrial predictive maintenance, with the potential to reduce downtime and maintenance costs while improving safety and reliability in manufacturing and other critical industries.",
            "steering_abstract": "**Abstract:**\n\nWe present *Conditional Variational Refinement (CVR)*, a radically new approach to text-to-image generation which produces high-resolution (1024 x 1024) photorealistic images via a text-conditioned refinement model, starting from a high-resolution random-noise image. The resulting images have no perceivable prompt artifacts while often exhibiting details that a human could easily mistake as photo-realism. We show that the same model, trained on just 2M captions, and using only a 22B parameter diffusion model as the base model, can now generate a diverse set of images of a given prompt of any style (e.g. \u201cneon style\u201d and \u201cgrainy 1980's TV effect\u201d etc.) or from a given reference image, or as a \u201cdoodle\u201d of the prompt. 1) To achieve this, we introduce two major technical advances, (1) A new base model called the *dual diffusion model* that samples images in two steps with two parallel models, instead of one. (2) A *conditioned-refinement* framework that samples the (remaining) 1/256-pixel-noise in the image via a model that is conditioned on the input text.  We use this framework to make a set of major technical contributions for text-conditioned refinement. We can now train refinement models on much less data, by sampling from our trained base model (no sampling needed at test time), by training the models on a single prompt per sample (as opposed to pairs of prompt/reference images), and by training the refinement models only on 1/512-pixel-noise instead of 1/256-pixel-noise. CVR gets strong baselines and strong test-time baselines that would not have been possible at ICML if done in isolation (e.g. it could get a 5.43 FID on the 1024x1024 COCO testset, vs 11.30 for the best (68B param) decoder-only baselines and 15.18 for the best 68B param paired decoder-refiner baseline). \n\nBecause the final prompt conditioned image only needs to be sampled from a single refiner, it can be sampled much faster than paired decoder-refiner baselines. Further, to produce a full 1024x1024 image at test time, only the base",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher degree of truly human creativity and potential impact. It proposes a \"radically new approach\" to text-to-image generation via Conditional Variational Refinement (CVR), introducing not only a novel framework but also two significant technical advances: the dual diffusion model and a conditioned-refinement process sampling pixel noise at unprecedented granularity. These innovations enable high-resolution, photorealistic image synthesis with less data and computational overhead, surpassing existing baselines in both quality (lower FID scores) and efficiency. The abstract communicates detailed technical depth and evidences a paradigm shift in generative modeling, with broad potential to influence the field of image generation. While Abstract A presents a solid and well-engineered new system (TGNet) for industrial predictive maintenance, its contribution is a more incremental improvement over current graph neural network applications, with less cross-domain resonance or revolutionary quality. Abstract B's advances are both substantial in rigor and more likely to be transformative for machine learning at large.",
            "was_flipped": false,
            "base_seed": 84,
            "steering_seed": 85
          },
          {
            "round": 3,
            "base_abstract": "Title: **End-to-End Differentiable Physical Simulation for Learning Generalizable Skills**\n\nAbstract: \nWe introduce a novel framework for end-to-end learning of generalizable physical skills using differentiable physical simulators. Our method seamlessly integrates deep neural networks with physics-based models, enabling the training of robots to perform complex and adaptive tasks in simulation, which are then effectively transferred to real-world environments. By leveraging automatic differentiation, our system efficiently optimizes both the policy and the underlying physics parameters, leading to significant improvements in learning speed and generalization capabilities. We demonstrate the efficacy of our approach through extensive experiments on a suite of physical manipulation tasks, including object assembly, sorting, and stacking, showing that our method outperforms existing reinforcement learning baselines by a substantial margin. Moreover, our framework allows for the interpretability of learned policies through the physics parameters, providing insights into the physical principles underlying the robot's behavior. This work opens new avenues for the development of intelligent robotic systems that can rapidly adapt to novel environments and tasks, marking a significant step forward in the intersection of deep learning and physical simulation.",
            "steering_abstract": "**Learning to Sample: Towards Trainable and Composable Likelihood-Free Inference**\n\nLikelihood-free inference (LFI) is a crucial tool for estimating the posterior distribution of parameters in generative models when the likelihood function is intractable. Despite promising recent work, LFI methods either require the likelihood to be available to some degree or become intractable to sample from the posterior due to the required rejection sampling. This paper proposes a novel, likelihood-free sampling-based training procedure to avoid the need for the likelihood. We learn a generative model over parameters and latent variables to match the joint distribution of the data and the parameters, with the latent variables to model the unnormalized likelihood or the data-generation mechanism. We train the model on the data by sampling from an initial generative model of the data and parameters, the joint distribution is then matched via rejection sampling. Importantly, to enable the model to be sampled from, we make the model a Gumbel-Top-k sample-able approximate Categorical model, i.e., the model outputs approximate logits which we then Gumbel-Top-k sample from. We show this allows the model to sample from the exact posterior, without the need for the intractable data-likelihood, and it is amenable to sampling-based composability. We conduct experiments on image data and show exact matching of the posterior on the ImageNet-1K validation set for the first time with LFI; we also find the model to be composable and show some initial composability experiments.\n\nOur approach enables generative-model based training of likelihood-free inference models to work in the practically-important case that the data-likelihood is intractable. We further make the model sample-able and show this allows the model to exactly match the posterior from a generative model and is amenable to composability. We show on standard and novel experiments that our approach is in fact very amenable to composability, on standard LFI experiments we show a first of its kind, exact match of the posterior of an LFI model on 1K images, and in composability experiments show that our model can exactly and exactly match the prior-likelihood posteriors and then exactly match the data-posterior given the exact prior-likelihood posteriors.",
            "winner": "abstract_b",
            "explanation": "Abstract A stands out as more truly human creative and impactful for several reasons. First, its originality is demonstrated through its innovative approach to likelihood-free inference (LFI) by proposing a trainable, composable, and exact posterior-matching method that does not require tractable likelihoods\u2014a central challenge in probabilistic inference. The technical rigor is high, involving non-trivial concepts such as Gumbel-Top-k sampling and composability in a likelihood-free context. The experiments reported are substantial, addressing a longstanding limitation in LFI (exact posterior matching on complex datasets like ImageNet-1K), and pushing the field forward both theoretically and practically.\n\nIn comparison, Abstract B, while impactful and clear in describing end-to-end differentiable physics-based simulation for generalizable robotic skills, is incremental relative to recent trends in differentiable simulation and reinforcement learning. It synthesizes ideas well but lacks the same degree of technical novelty and does not demonstrate the same depth of insight into a hard technical barrier.\n\nOverall, Abstract A demonstrates deeper creativity by finding a practical and theoretically sound solution to a fundamental barrier in LFI, with important implications for generative modeling and probabilistic reasoning.",
            "was_flipped": true,
            "base_seed": 86,
            "steering_seed": 87
          },
          {
            "round": 4,
            "base_abstract": "**Abstract:**\n\nIn this paper, we present a novel framework for addressing the challenge of learning robust and efficient representations in deep neural networks, particularly in high-dimensional and noisy environments. Our approach, dubbed *Adaptive Robust Feature Learning (ARFL)*, introduces a dynamic mechanism for automatically adjusting the network's sensitivity to input perturbations and noise. ARFL leverages a novel adaptive weighting scheme that dynamically balances the importance of feature robustness and accuracy during training, enabling the network to adapt to varying levels of input uncertainty.\n\nTechnically, ARFL integrates a robustness-aware loss function with an adaptive normalization layer, which modifies the feature space to be more resilient to perturbations. This framework is designed to be compatible with a wide range of deep learning architectures, including convolutional neural networks and transformer models, without requiring significant modifications to the existing network architecture or training pipeline.\n\nOur empirical evaluation demonstrates that ARFL outperforms state-of-the-art methods across multiple benchmark datasets, including image classification, natural language processing, and time-series forecasting tasks. Specifically, ARFL achieves a 10% improvement in robust accuracy on the ImageNet-C benchmark, a 15% reduction in error rate on noisy text classification, and a 20% improvement in prediction accuracy on financial time-series data under noise conditions.\n\nMoreover, we provide a detailed theoretical analysis that establishes the robustness and efficiency guarantees of ARFL. The framework is grounded in novel mathematical principles that ensure it scales well with increasing network complexity and data dimensionality. The significance of our work lies in its potential to substantially enhance the robustness and generalizability of deep learning models in real-world applications, where data quality and perturbations are often unpredictable.\n\nThis research not only advances the state-of-the-art in robust feature learning but also opens up new avenues for further exploration in adaptive learning and robustness in machine learning.",
            "steering_abstract": "**Abstract:**\n\nWe introduce a new algorithm, **Contrastive Latent Inference (CLI)**, for learning the parameters of a flexible generative model from incomplete data, using contrastive examples. Given an observed data point, the algorithm samples a contrastive point according to a contrastive model that can be learned from complete data. This model may be different from the generative model of interest (but is usually a simpler model). If the contrastive model prefers to generate the observed data point (according to log-likelihood) compared to the sampled point, the generative model's parameters are updated to increase this preference. The contrastive model is given a loss that penalizes it for making incorrect guesses. We can learn the generative model by contrastive learning (CLI) starting from a flexible generative model, even if it is parametrized by a very complicated implicit generative model like a diffusion model or an auto-regressive model. We prove that this is a maximum likelihood update, if we take the limit of a large number of contrastive samples. Our experiments show CLI substantially outperforms alternatives, using an implicit model with 1 billion parameters, on tabular data, images, and language, achieving 58.88 top-1 accuracy on ImageNet and 12.46 bits-per-dimensional on 1-bit quantized Text117M. We also present evidence that contrastive examples are helping the model to attend the right places in its generative model. Overall, this paper presents a new learning procedure and explains when it can help, as contrastive examples can get the right answer at lower computation cost than generating the answer directly.\n\n**Why this is a winning abstract:**\n\n1. **Significance**: The paper provides a simple but powerful algorithm that can be used for training generative models. It can be used with any generative model, is simple to implement, and matches the state of the art on all tested downstream tasks.\n\n2. **Technical depth**: This is a paper about theory, presenting an algorithm that is provably as powerful as maximum likelihood learning as a first order update. It is a simple but powerful idea. The authors prove that the first order update that we take to train a generative model using contrastive data is equivalent to a maximum likelihood update. They also explain how to use this update to train a large implicit generative model that can be evaluated only via sampling.  We show it is fast, it can be applied to a very large generative model,",
            "winner": "abstract_b",
            "explanation": "Abstract A clearly demonstrates greater human creativity and impact for several key reasons. First, it introduces an innovative, broadly applicable algorithm\u2014Contrastive Latent Inference (CLI)\u2014that leverages contrastive examples in generative modeling. The approach is conceptually novel because it marries contrastive learning with maximum likelihood estimation in the context of very flexible, even implicit generative models (including diffusion and autoregressive models), and it is supported by both rigorous theoretical analysis and empirical results across diverse modalities (tabular data, images, language). The advancement seems especially creative in its use of contrastive models to train another generative process, showing insight into training dynamics and efficiency.\n\nBy contrast, Abstract B, though technically sound and promising in terms of robust representation learning (ARFL), primarily offers advancements within the well-trodden domain of robust feature learning via adaptive mechanisms. While it introduces an adaptive weighting and normalization strategy, this general area of adaptive robustness in neural networks is more incremental, and the creative leap is less pronounced than in Abstract A. The technical contribution, though valuable, reads as an expected next step rather than a novel paradigm shift.\n\nFurthermore, Abstract A\u2019s writing is clear, focused, and makes explicit claims about algorithmic simplicity, universality, theoretical insight, practical efficacy, and broader applicability, which will likely catalyze new lines of research. Its combination of novelty, rigor, broad impact potential, and creative conceptual synthesis makes it the clear winner.",
            "was_flipped": true,
            "base_seed": 88,
            "steering_seed": 89
          },
          {
            "round": 5,
            "base_abstract": "Abstract:\n\nThis paper introduces a novel framework for learning probabilistic models from streaming data with provable guarantees on scalability and accuracy. Our method, named StreamingProbabilisticLearning (SPL), is designed to address the challenges posed by real-time data processing in dynamic environments. SPL leverages a hybrid approach combining online learning techniques with advanced probabilistic inference algorithms, enabling efficient and robust model updates in the face of streaming data.\n\nWe develop a novel online inference algorithm that achieves near-optimal statistical efficiency while maintaining computational efficiency. This algorithm is supported by a rigorous theoretical analysis, which proves that SPL converges to the optimal model parameters with high probability under mild assumptions. Empirically, we demonstrate the superiority of SPL over state-of-the-art methods on a variety of real-world streaming datasets, including financial time series, sensor networks, and social media data. Our experiments show significant improvements in prediction accuracy and computational efficiency, particularly in high-dimensional and high-frequency data settings.\n\nFurthermore, we extend SPL to handle streaming data with uncertainty and missing values, which are prevalent in many real-world applications. This extension allows SPL to maintain robust performance even under data incompleteness, a critical requirement in practical scenarios. We validate our claims through comprehensive experiments on synthetic and real-world datasets, including comparisons with baseline methods and ablation studies to identify the key contributions of our approach.\n\nThe proposed framework not only advances the theoretical understanding of streaming probabilistic learning but also provides a practical solution for real-time data processing, with potential applications in a wide range of domains such as finance, healthcare, and environmental monitoring.",
            "steering_abstract": "**Abstract:** \n\nA fundamental challenge in machine learning is developing algorithms that can learn efficiently from as few examples as possible. In this paper, we present a new approach for learning in the setting of *few-shot learning*. In this setting, a model is required to solve a task it has never seen, from as few as a single example. We present *Few-Shot Meta-Learning via Implicit Sampling* (FISM), a new approach to few-shot learning that uses implicit sampling. Implicit sampling samples from a distribution that implicitly encodes the gradient of an optimal solution of a different loss. We use implicit sampling to sample a distribution of few-shot predictions to use as meta-parameters that are used to predict the few-shot loss. We show that for the most common meta-learning algorithms (i.e. Model-Agnostic Meta-Learning (MAML) and Prototypical Networks (PN)), this leads to a surprising new algorithm (I-Sampling MAML and I-Sampling PN) that provably minimizes the few-shot loss while remaining tractable to run. We show empirically that this simple idea, despite its surprising theoretical guarantee, is a novel way to improve performance on a large array of few-shot tasks: Across the CLEVR, MiniGPTZero, and miniImageNet few-shot datasets, we show in-sample performance is 3-12x better than the best previous approach and, when out-of-sample, it achieves state-of-the-art performance, a 10% relative gap to human performance.",
            "winner": "abstract_b",
            "explanation": "Abstract B stands out as the more creative and impactful work for several reasons:\n\n1. Novelty and Originality: Abstract B introduces a genuinely new conceptual approach\u2014Implicit Sampling in Few-Shot Meta-Learning. Unlike most classic few-shot or meta-learning techniques, the notion of sampling from a distribution that encodes gradients as meta-parameters and then leveraging this for few-shot loss minimization is an original twist. The abstract highlights not just empirical results but a surprising theoretical guarantee, further underscoring its innovation.\n\n2. Technical Rigor and Depth: B provides technical insight by detailing how the implicit sampling modifies and enhances Meta-Learning algorithms like MAML and Prototypical Networks. The explanation skims the mathematical mechanism but suggests deep theoretical thought, especially with the claim of provable loss minimization.\n\n3. Clarity and Quality of Writing: B's writing is tight and directly communicates the key findings and novelty, with impactful statistics (\"in-sample performance is 3-12x better\") and clear connections to the broader challenge of few-shot learning.\n\n4. Potential Impact: Few-shot learning remains a grand challenge in machine learning, and an algorithm as described in B\u2014combining surprising theory and striking improvements\u2014could have significant impact in areas from natural language understanding to computer vision.\n\n5. Human Creativity and Insight: The creation of a theoretically sound yet simple idea, leading to large performance jumps, exemplifies a leap akin to genuine human creativity. The conceptual rethinking of how distributional knowledge can guide few-shot tasks is both nuanced and original.\n\nIn comparison, Abstract A presents a solid and valuable contribution\u2014an improved scalable streaming probabilistic model learner. However, the ideas are more evolutionary and less fundamentally novel, combining known techniques in an efficient blend rather than opening a new paradigm. \n\nTherefore, abstract_b demonstrates greater creativity, theoretical depth, and potential impact, making it the superior choice.",
            "was_flipped": false,
            "base_seed": 90,
            "steering_seed": 91
          },
          {
            "round": 6,
            "base_abstract": "**Abstract:**\n\nWe present a novel framework for Learning Compositional Skills through Hierarchical Reinforcement Learning (HRL) and Intrinsic Motivation, which we term Compositional Skill Learning (CSL). Our approach leverages a deep hierarchical reinforcement learning architecture, where lower-level policies learn basic skills, and higher-level policies dynamically combine these skills to solve complex tasks. Crucially, we introduce a novel intrinsic motivation mechanism, called Skill-based Curiosity (SB-Curiosity), which drives the agent to explore and acquire a diverse set of skills by predicting the utility of skills in future tasks. This mechanism is grounded in a meta-reward scheme that dynamically adjusts the intrinsic motivation based on the current task context, ensuring that the agent prioritizes learning skills that are most relevant to the task at hand. \n\nOur framework is evaluated across a suite of challenging robotics and navigation tasks, including the ProcGen benchmark and a custom 3D navigation environment. We demonstrate that CSL outperforms state-of-the-art HRL methods in terms of both task success rate and sample efficiency. Notably, our approach achieves a 30% improvement in task success rate and a 40% reduction in training time compared to the baselines. Moreover, we provide a detailed analysis of the learned skill repertoire, showing that the agent has acquired a rich set of fine-grained skills that can be flexibly combined to solve unseen tasks. This work represents a significant step forward in developing autonomous agents capable of learning and generalizing from a wide range of skills in complex environments, paving the way for more sophisticated and adaptable AI systems.",
            "steering_abstract": "**Unsupervised Domain Adaptation with Latent-Variable Energy-Based Models**\n\n*Kevin Xu, Aravind Rajeswaran, Zhizhou Ren, Robin Jia, Yixin Wang, Percy Liang*\n\n*Stanford University, UC Berkeley, CMU*\n\n**Abstract:** Unsupervised domain adaptation (UDA) leverages a large labeled source dataset and a small but labeled target dataset, in the absence of any source-target labeled data, to achieve good performance on the target domain. In this work, we propose a simple new approach: train an un-normalized conditional density (an energy-based model) to fit the source dataset, then use it to estimate the log-likelihood of the target samples to score each label, and break ties randomly. This approach is simple yet surprisingly effective. We study it in depth, including proving a necessary condition for success and showing the best when paired with a simple classifier. On popular benchmarks, our simple new approach (UBF) matches strong existing methods on 2/3 of baselines and outperforms them on 1/3, and it achieves state-of-the-art on the largest real dataset (COCO). Our simple new approach is simple, fast, and only requires code to sample from a conditional distribution (e.g. sampling a caption conditioned on an image) and is simple to optimize.",
            "winner": "abstract_a",
            "explanation": "Abstract B demonstrates greater human creativity and impact due to its novelty in both architectural design and the introduction of an original intrinsic motivation mechanism (Skill-based Curiosity). The work proposes a hierarchical reinforcement learning system that autonomously discovers and composes skills in a way not previously achieved, substantiated by a context-sensitive curiosity mechanism. This allows agents to adapt and generalize in complex environments with high efficiency and transferable skill repertoires. The paper claims a significant advancement in both sample efficiency and task success rates over strong baselines, indicating strong potential impact. Furthermore, the writing is clear, technically rigorous, and thoroughly connects the novel ideas to potential real-world impact, making it a stand-out in terms of creativity. By contrast, Abstract A, while solid and technically competent, offers a \"simpler\" take on existing domain adaptation methods and appears less ambitious in scope and transformative potential. Therefore, Abstract B is more genuinely creative and impactful.",
            "was_flipped": true,
            "base_seed": 92,
            "steering_seed": 93
          },
          {
            "round": 7,
            "base_abstract": "Abstract:\n\nThis research paper introduces a novel framework, Adversarially Regularized Multi-View Learning (ARML), which significantly enhances the robustness and generalization of machine learning models across diverse data modalities. ARML leverages adversarial training to dynamically adjust the weight assignments among multiple views of data, ensuring that the model's predictions are consistent and resilient to perturbations across different views. This framework addresses the limitations of existing multi-view learning methods, which often struggle to effectively utilize heterogeneous data sources and maintain model robustness under adversarial attacks.\n\nOur contributions include: (1) a novel adversarial regularization mechanism that dynamically balances the contribution of each data view, (2) a robust multi-view loss function that ensures cross-view consistency, and (3) an efficient optimization algorithm that scales to large-scale datasets. We demonstrate the effectiveness of ARML through extensive empirical evaluations on benchmark datasets for image classification, natural language processing, and tabular data analysis, where it consistently outperforms state-of-the-art methods in terms of accuracy, robustness, and generalization.\n\nMoreover, we conduct a thorough analysis of the framework's interpretability and show that ARML can provide insights into the relative importance of different data views, enabling more informed feature selection and data integration strategies. This work not only advances the state-of-the-art in multi-view learning but also paves the way for more robust and versatile machine learning models in real-world applications.",
            "steering_abstract": "**Towards Near-optimal Neural Calibration with a Per-Example Lagrange Multiplier**\n\nDespite remarkable empirical success, existing methods for neural network calibration (e.g., BQI and L2-FLIP) fall short in minimizing the worst-case expected (or fixed) calibration error for the entire output space. They tend to over-calibrate most examples and under-calibrate a few. In this work, we show that the problem can be solved in a *near-optimal* manner with a per-example Lagrange multiplier (PLM) approach, as the solution is the argmax of a simple per-example loss. Specifically, our new PLM approach minimizes a simple non-convex per-example loss, with the exact Lagrange multiplier chosen by the per-example argmax. We prove that the per-example argmax achieves near-optimal expected calibration error (ECE) on a held-out test distribution. We also prove an upper bound on the worst-case ECE. We find on 6 test distributions, the PLM approach achieves a 10-40% near-optimal reduction of the worst-case ECE over the top two baselines (BQI and L2-FLIP). Moreover, for calibration, the PLM approach is 2x faster to converge than either of the two baselines. For confidence calibration, it reduces the test NLL by 1-25%. We note that PLM is 2x as simple as L2-FLIP and faster. \n\n*Note: ECE is a good proxy for the fixed worst case calibration error as it\u2019s within 10% of the fixed worst case error on 249 of the 320 splits we checked, with a mean absolute diff of 1.2%. ECE (or its 1/100th version) is used in our experiments to make the claims precise; we plot the test worst-case calibration error in the paper plots. L2-FLIP is the leading baseline.*",
            "winner": "abstract_b",
            "explanation": "Abstract A introduces a genuinely original solution\u2014per-example Lagrange multipliers\u2014to a recognized challenge in neural network calibration. The novelty is clear: instead of improving existing broad or heuristic approaches, the paper proposes and mathematically analyzes a per-example optimization scheme. This approach is both creative and targeted, offering provable improvements and specific complexity and convergence benefits over strong baselines. The technical rigor is evidenced by theoretical guarantees (near-optimality, bounds on calibration error) and concrete empirical results, while the clarity of writing allows a reader to clearly track the motivation, method, and results.\n\nAbstract B, while innovative, presents adversarial regularization in multi-view learning\u2014a field where adversarial and multi-view techniques have previously merged in different forms. Its contributions, though solid and practical, feel like well-executed evolutionary advances, not transformative leaps. Its level of technical detail and originality appears less substantial than A\u2014adversarial balancing across data views is a logical next step, but not a fundamentally new idea. The writing is clear, but the impact and genuine insight seem less profound than the leap represented by per-example Lagrange multipliers for calibration in Abstract A.\n\nTherefore, Abstract A demonstrates greater technical originality, impactful insight, and the hallmarks of truly creative human research.",
            "was_flipped": true,
            "base_seed": 94,
            "steering_seed": 95
          },
          {
            "round": 8,
            "base_abstract": "**Abstract:**\n\nWe present a novel framework, **Dynamic Contrastive Estimation (DCE)**, which significantly advances the state-of-the-art in unsupervised representation learning for sequential data. DCE leverages a dynamic contrastive framework to learn robust representations by contrasting pairs of samples within sliding windows, dynamically adjusting the temporal context based on the sequence structure. Our approach generalizes existing methods, such as Contrastive Divergence and Contrastive Predictive Coding, by incorporating dynamic temporal dependencies that are crucial for capturing long-range dependencies in sequential data. \n\nWe demonstrate the efficacy of DCE through extensive experiments on various benchmark datasets, including natural language processing tasks (e.g., language modeling and sentiment analysis) and time-series analysis (e.g., stock price prediction and anomaly detection). Our method consistently outperforms existing state-of-the-art methods, achieving up to a 20% improvement in metrics such as perplexity and F1 score. Furthermore, we conduct ablation studies to dissect the impact of each component of DCE, providing insights into the robustness and generalizability of our framework. \n\nTheoretical analyses support the proposed framework, showing that DCE effectively minimizes the negative correlation between representations in different sliding windows, thereby enhancing the learned representations' ability to capture complex temporal patterns. We also evaluate the computational efficiency of DCE, demonstrating that our method is scalable and can be implemented efficiently on standard hardware. Overall, DCE represents a significant advance in unsupervised representation learning for sequential data, with broad applicability across various domains.",
            "steering_abstract": "**Attention is Not Enough: A Unifying Framework for Disentangled Representations**  \n\nUnderstanding and leveraging disentangled representations has long been a holy grail of machine learning. This is because disentangled representations are a cornerstone of the recent success of methods such as VAEs and GANs, as well as the holy grail for methods such as inverse problems and imitation learning. However, it is unclear how to make these representations disentangled and how to make them useful. In this paper, we make a novel claim: the issue is not how to make representations be (partially) disentangled, but how to make representations be useful (for some task, for some utility function). To do this, we propose the Disentangled Utility Framework (DUF), which unifies 11 disparate but overlapping ideas of disentanglement: making a representation useful, disentangled, or sparse; or of doing each with an encoder and a decoder (disentangled-encoder, encoder, or disentangled-decoder). DUF is the first framework to unite disentangled representations from three perspectives (usefulness, structure, and sparsity), instead of from two as other works do (structure and sparsity). We do not claim that one of these perspectives is more important than the others, but that they should all be considered at the same time. Crucially, DUF also makes all these disentangled representation ideas directly useful for a task by unifying two ideas: a disentangled representation is an abstract representation of the data, and when the data is then used by a task, a representation will be useful when there is a large conditional effect on the task. This conditional effect can be learned by a utility function which is used to measure the disentangled representation. We show through theory and experiments that DUF is more aligned with the goals of having a useful representation than the other popular ideas for making representations disentangled. In the other methods, you can have the goal of being disentangled and useful, but can't get a (disentangled) representation that is (useful) if you try to have disentangled and useful goals at the same time. Our approach, DUF, makes these goals work in concert, giving the best of both worlds, in one approach. DUF is also one of the first papers to give a single framework, rather than a single approach, for disentangled representations. This is in stark contrast to existing work, that give the",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates a higher degree of human creativity and potential impact. It addresses a long-standing challenge in machine learning\u2014disentangled representations\u2014not by proposing an incremental algorithm, but by critically reconceptualizing what the goal should be (usefulness over mere disentanglement). The proposal of a unifying framework (DUF) to bring together previously disparate ideas and perspectives (usefulness, structure, sparsity) is both ambitious and theoretically deep. The narrative is novel, as it emphasizes unifying perspectives and actionable utility rather than yet another method. This conceptual leap, alongside a framework that connects theory and practical utility, points to original thinking that could meaningfully shift how the community approaches representation learning. Abstract B, while technically solid and incremental in improving contrastive methods for sequential learning, does not reach the same level of conceptual reframing; it extends existing paradigms rather than challenging foundational assumptions. Thus, Abstract A stands out as more creative and potentially impactful.",
            "was_flipped": true,
            "base_seed": 96,
            "steering_seed": 97
          },
          {
            "round": 9,
            "base_abstract": "Title: \"Dynamic Graph Neural Networks for Adaptive Hierarchical Representation Learning\"\n\nAbstract:\nThis paper introduces a novel class of Dynamic Graph Neural Networks (DGNNs) that dynamically adjust their hierarchical structure in response to evolving data. Building upon the existing framework of Graph Neural Networks (GNNs), DGNNs offer a significant advancement in representation learning on graph-structured data by leveraging the power of dynamic programming and reinforcement learning. Our approach dynamically partitions the graph into a hierarchy of subgraphs, allowing each subgraph to be processed by a specialized GNN layer. This hierarchical structure evolves over time, enabling the network to adapt to changing graph topologies and node features.\n\nOur contributions are threefold. First, we propose a novel mechanism for dynamically partitioning graphs that balances computational efficiency and model expressiveness. Second, we develop a reinforcement learning-based framework to optimize the partitioning process, ensuring that the resulting hierarchical structure maximizes representation quality. Third, we introduce a self-attention mechanism that allows each subgraph to selectively propagate information, enhancing the model's ability to capture long-range dependencies in the graph.\n\nWe demonstrate the efficacy of DGNNs on a variety of graph-based tasks, including node classification, link prediction, and community detection, showing significant improvements over state-of-the-art methods in both accuracy and scalability. Our results are particularly compelling in dynamic graph settings, where DGNNs outperform static GNNs by adapting to real-time changes in graph structure. This work paves the way for more efficient and adaptive GNNs, with potential applications in areas such as social network analysis, recommendation systems, and biological network modeling.",
            "steering_abstract": "**Abstract:**\n\nWe present a new framework for addressing long-range alignment, the critical challenge of ensuring large language models behave ethically. We propose a method called \"Conditional Inversion\" that, given an aligned prompt, retrains the model to produce a more aligned response given an aligned prompt. Conditioning on a sufficiently aligned prompt can act as a kind of safety \"gold seal\", allowing us to use the model for a few aligned rewrites that can be used to \"improve\" the response, and, on a long enough prompt, can potentially fully re-align the model. We show that it is possible to safely re-align a large model to any prompt (using up to 10k tokens of aligned prompt), to be aligned in the way the aligned prompt is. We show on a 1.3B parameter model that a prompt of about 10k tokens can re-align the model so it behaves in line with that 10k token prompt. To make alignment tractable, we propose using models like Qwen or Vicuna to get a much smaller model to the correct alignment, then get those smaller models to re-align the large model, as the smaller model only has to remember a small aligned prompt. Given the tractability of Qwen/Vicuna, this can be a new class of alignment tool, a \"prompt-aligned smaller model\" that can be cheaply used to get a large model aligned in any way. Further, a smaller prompt-aligned model may be easier to get aligned, and so could be used to get alignment in a chain, a model could first make a smaller model aligned, and then that smaller model in turn re-aligns a larger one. We find that Qwen re-aligning a 1.3B parameter Qwen takes about 24 hours to get to 10k token aligned prompt, and that this takes the Qwen model to behave as the 10k token aligned prompt. We find that on a 1.3B parameter Qwen model, that a 10k token aligned prompt can get the model to act in accordance with that aligned prompt in 1-2 tokens. On the 1.3B parameter Qwen we find that a 40-100 token aligned prompt can be used to make the Qwen model align to the prompt, on a 13B parameter Qwen model, a 35-100 token prompt can be used. We find that some prompts that are aligned (e",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates greater human creativity and impactful ideas due to several reasons:\n\n1. **Novelty and Originality**: The idea of dynamically evolving hierarchical structures in graph neural networks using both dynamic programming and reinforcement learning is both novel and intellectually sophisticated. The combination of partitioning, reinforcement learning, and self-attention for adaptive graph processing is genuinely creative.\n\n2. **Technical Rigor and Depth**: The abstract clearly describes a well-motivated methodology with specific technical contributions (dynamic partitioning, reinforcement learning for hierarchy optimization, and self-attention). The design and interplay of these techniques suggest a deep engagement with core research challenges in graph representation learning.\n\n3. **Clarity and Writing Quality**: The abstract is well-organized, logically flowing from motivation to contributions to results and impact, summarizing the technical advances and articulating their significance clearly.\n\n4. **Potential Impact**: By targeting dynamic graph structures\u2014a substantial challenge in real-world applications\u2014the paper's contributions have broad implications for multiple domains. The approach to adapt both hierarchy and representation learning could advance fields like social network analysis and bioinformatics.\n\nIn contrast, Abstract B, while conceptually interesting, suffers from less clarity and more repetition, with ideas less crisply presented. Its main approach (prompt-based alignment leveraging smaller models) is exciting but less well-developed or articulated compared to the concrete technical advances in Abstract A. Some details seem muddled, and the broader impact is less clearly stated.\n\nTherefore, Abstract A is the winner due to its clear creativity, technical rigor, strong organization, and greater potential impact.",
            "was_flipped": false,
            "base_seed": 98,
            "steering_seed": 99
          },
          {
            "round": 10,
            "base_abstract": "Abstract:\n\nWe introduce a novel, differentiable architecture for learning compositional knowledge in deep neural networks, termed the Compositional Memory Network (CMN). Unlike existing memory-based models that rely on fixed-size memory banks or static embedding schemes, CMN dynamically constructs and manages a hierarchical memory structure through a learnable memory composition mechanism. This architecture enables the model to adaptively represent complex relational patterns and hierarchical dependencies in data, significantly enhancing its capability to solve structured reasoning tasks. \n\nOur key contributions include: (1) A novel differentiable memory composition mechanism that allows the network to dynamically construct and modify its memory structure, (2) A novel learning objective that encourages the network to optimize both memory efficiency and representation quality, and (3) Theoretical analyses that demonstrate the CMN's ability to generalize well to unseen data and handle hierarchical compositional tasks more effectively than previous models.\n\nEmpirical evaluations on benchmark datasets such as Mini-Imagenet, CUB, and Conceptual Captions show that CMN outperforms state-of-the-art models by a substantial margin. Notably, CMN achieves a 10% relative improvement in accuracy on the most challenging few-shot learning tasks. We also present qualitative results that reveal CMN's ability to capture complex relational structures and hierarchical compositions, which are crucial for solving intricate reasoning problems. Our work sets a new benchmark for learning compositional knowledge in neural networks and paves the way for more advanced and efficient models in the future.",
            "steering_abstract": "**Abstract:**\n\nWe introduce **Neural Constructions (NC)**, a novel framework that constructs the parameters of large neural networks as a differentiable sequence of deterministic computation steps. Each step is conditioned on the previous ones, as if building up an analog construction starting from an initial seed. We introduce the *construction guidance* objective, which incentivizes the model to construct its parameterization using a short sequence of steps, favoring smaller and more efficient models, or models with more similar parameters. In our framework, the construction history of a model can be seen as an implicit compression of the model's parameters: smaller sequences of steps can capture the same distribution of parameters. This is the first known framework that can construct any class of large models from a short sequence of steps. We show that Neural Constructions can be trained with construction guidance, and that its resulting models are as accurate as standard trained models, all while being much smaller and sharing much of their weights. Our experiments show 8x smaller and 2.5x faster models with near full accuracy, with up to 16x smaller models than the original and the same accuracy as standard trained models, all constructed with a sequence of just a few steps. Further, we find that these smaller models are still as good for transfer-learning and distillation as much larger models. Thus, Neural Constructions enable the construction of better smaller models with near full accuracy, which are faster to generate from scratch and faster to run at inference, and which can be further used to distill to very small models. Overall, we hope that Neural Constructions will enable models to be constructed and used in more settings (e.g. on edge devices), more quickly, and more efficiently.",
            "winner": "abstract_b",
            "explanation": "Abstract B (Neural Constructions) demonstrates a higher level of genuine human creativity and potential impact. Its proposal to construct large neural networks through a sequence of differentiable steps\u2014compressing and parameterizing models in a novel, interpretable way\u2014pushes the boundaries of how we think about model architecture, compression, and deployment. The idea of constructing any class of large models from just a short sequence of steps is distinctly original, and its implications for efficiency, scalability, and portability (especially for edge devices and faster deployment) are profound. The writing is clear, well-organized, and communicates both the technical novelty and the broader significance. In contrast, while Abstract A (Compositional Memory Network) introduces an interesting architecture for compositional reasoning, its approach is more incremental, building on ongoing research in memory-based neural architectures. Abstract B stands out for both originality and the breadth of its potential impact, with creative connections to model compression, transfer learning, and real-world usability.",
            "was_flipped": false,
            "base_seed": 100,
            "steering_seed": 101
          }
        ]
      },
      {
        "real_abstract": {
          "title": "Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators",
          "abstract": "Optimizing neural networks with loss that contain high-dimensional and high-order differential operators is expensive to evaluate with back-propagation due to \ue23b(dk) scaling of the derivative tensor size and the \ue23b(2k\u22121L) scaling in the computation graph, where d is the dimension of the domain, L is the number of ops in the forward computation graph, and k is the derivative order. In previous works, the polynomial scaling in d was addressed by amortizing the computation over the optimization process via randomization. Separately, the exponential scaling in k for univariate functions (d=1) was addressed with high-order auto-differentiation (AD). In this work, we show how to efficiently perform arbitrary contraction of the derivative tensor of arbitrary order for multivariate functions, by properly constructing the input tangents to univariate high-order AD, which can be used to efficiently randomize any differential operator. When applied to Physics-Informed Neural Networks (PINNs), our method provides >1000\u00d7 speed-up and >30\u00d7 memory reduction over randomization with first-order AD, and we can now solve emph{1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU}. This work opens the possibility of using high-order differential operators in large-scale problems."
        },
        "comparisons": [
          {
            "round": 1,
            "base_abstract": "Abstract:\n\nWe present a novel framework for enhancing the robustness and generalization of deep learning models through a technique we call **Hierarchical Adaptive Regularization with Uncertainty-aware Loss Adjustment (HARULA)**. This method dynamically adjusts regularization strengths based on uncertainty estimates, adapting to the complexity of the training data and mitigating overfitting without sacrificing model performance. Our framework integrates a Bayesian uncertainty estimator with a hierarchical regularization strategy, enabling more efficient and effective learning in complex and high-dimensional datasets. \n\nWe empirically demonstrate the superiority of HARULA on a wide range of benchmark datasets, including ImageNet, CIFAR-10, and MNIST, achieving state-of-the-art performance in terms of both accuracy and robustness to adversarial attacks. Notably, HARULA shows consistent improvements in generalization, particularly on out-of-distribution data, and can be seamlessly applied to a variety of neural network architectures, including convolutional neural networks, transformers, and residual networks. \n\nOur theoretical analysis provides a rigorous justification for the effectiveness of HARULA, showing that the proposed method optimally balances the trade-off between fitting the training data and maintaining generalizability. This work significantly advances the field of deep learning by offering a practical and theoretically grounded solution to the long-standing challenge of robust and generalizable model training.",
            "steering_abstract": "Optimizing the Intractable: Exact and Approximately Optimal Algorithms for Intractable MDPs\n\nIn many partially observable MDPs (POMDPs) and POMDP fragments, the exact optimal policy is intractable. We present an optimal exact algorithm, and show that for a fragment of POMDPs, we have a faster approximation. This contrasts with the usual situation where we can either get approximate or exact suboptimal policies. These faster algorithms are possible using a new exact or approximate reduction from POMDPs to POMDP fragments. To do so, we introduce exact or approximately optimal algorithms to this fragment. We conjecture that this is tractable, or in other words, all POMDPs are \u201cessentially\u201d of this form (a POMDP is essentially the fragment if its solution can be reduced to that of the fragment). We prove that all MDPs and 1-step-observation and action fragments are essentially this fragment. We also show that the set of fragment solutions is NP-hard to approximate. \n\nThis problem and its study could be seen as a case study in two things. It is a clean concrete problem that has a natural language, but is also general and intractable. So it is like a clean concrete problem about a clean concrete problem (like an interesting 8-puzzle or a very small, minimal undecidable Turing machine that are still in some ways clean and interesting, respectively). The work is also general because the result is a reduction that works with all kinds of fragments. And though the problem is very small and simple, it is intractable. This intractability gives an opportunity to study tractable cases in terms of tractability parameters. We did this, but are also leaving a challenge of finding an interesting tractable fragment for which no one has found an exact optimal algorithm.",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher level of human creativity and deep insight. While Abstract A presents a technically solid extension of established methods (adaptive regularization, Bayesian uncertainty) and likely offers practical impact, its innovation appears incremental and largely fitted into standard paradigms in deep learning research. Abstract B, by contrast, tackles the foundational intractability of POMDPs and introduces new algorithmic reductions. Its discussion reaches a level of theoretical abstraction, posing open conjectures and linking concrete problem fragments to broader questions of computational complexity. The writing, while more nuanced and less conventional, expresses a genuine engagement with mathematical depth and invites further exploration\u2014a hallmark of impactful academic creativity. The focus on tractability parameters and the explicit challenge to future researchers further elevates its potential significance. For these reasons, Abstract B is judged more truly human creative and impactful.",
            "was_flipped": false,
            "base_seed": 102,
            "steering_seed": 103
          },
          {
            "round": 2,
            "base_abstract": "**Abstract:**\n\nThis paper presents a novel framework for differentiable physics simulation, enabling the joint training of physics models and neural network controllers within a unified optimization process. Our approach, Differentiable Physics-enhanced Control (DPC), leverages adjoint-based optimization to efficiently backpropagate through complex physical simulations, facilitating the simultaneous optimization of both the physical and control parameters. By integrating physics simulation into the gradient computation pipeline, DPC allows for the training of controllers that can adapt to dynamic environments and predictably interact with complex physical systems.\n\nWe demonstrate the effectiveness of DPC through a series of rigorous experiments on various physical simulation tasks, including robotic manipulation and autonomous vehicle navigation. Our method outperforms existing approaches in terms of learning efficiency and generalization capabilities, achieving state-of-the-art performance on benchmarks such as MuJoCo and Carla. Notably, DPC enables the design of controllers that are not only effective but also interpretable, providing insights into the decision-making processes during physical interactions.\n\nMoreover, we contribute a novel adjoint-based backpropagation algorithm tailored for sparse and discontinuous gradients often encountered in physics simulations. This algorithm significantly reduces the computational overhead and memory footprint of the optimization process, making DPC scalable to high-dimensional and complex physical systems.\n\nThe significance of our work lies in its ability to bridge the gap between traditional physics simulation and modern machine learning, paving the way for more robust and adaptive AI systems that can operate in dynamic and unpredictable environments. Our approach sets a new standard for the design of control systems that seamlessly integrate with physical understanding, opening up new avenues for research in robotics, autonomous systems, and beyond.",
            "steering_abstract": "```markdown\n``-title: A New Mathematical Framework for Understanding and Improving Generalization in Neural Networks\n-abstract: While the generalization mystery of deep neural networks has remained a longstanding and fundamental open problem in ML, we propose a new mathematical framework grounded in a novel object, the _pseudogradient_, that rigorously shows for the first time that we can exactly calculate the generalization gap. While the pseudogradient is formally defined over the continuum of function space, we give an algorithm for the pseudogradient and empirically show that it can exactly calculate the generalization gap to within a few percent on several neural network models trained on datasets of MNIST, CIFAR-10, CIFAR-100 and IMDB. We also prove that the generalization gap is monotonically nonincreasing (and sometimes sharply decreases) with the amount of data, which resolves a longstanding open problem in learning theory. We also prove that the generalization gap is monotonically nonincreasing (and sometimes sharply decreases) with the amount of data, which resolves a longstanding open problem in learning theory. These results rigorously hold for the continuum of function space, that is, for every function (regardless of whether it's in the hypothesis class or not) with the continuum of possible data. We also prove that the generalization gap monotonically nonincreases (and sometimes sharply decreases) with the amount of data, which resolves a longstanding open problem in the learnability theory. The pseudogradient naturally leads us to a novel optimization algorithm, which is much faster and better at converging to a better generalizing solution. We also give a simple and clean explanation as to why some overparameterized models can generalize so well: their large size causes them to have many pseudogradient vectors, and hence have more ways to get to a solution with a smaller generalization gap. We conclude that we now understand the generalization mystery of neural networks much more precisely and clearly than before, and that the new objects and math in this paper will lead to future major developments and progress. \n``-title: Bridging the Generalization Gap in Deep Learning\n-abstract: In spite of the explosion of deep learning applications, its generalization mystery remains a central, fundamental, and unsolved open problem in the ML community, despite the explosion of effort and work over 30 years. We give the first mathematical object and formalism that exactly calculates the generalization gap (or the gap in expected and empirical loss) exactly for",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher level of genuine human creativity and impact in terms of foundational insight and potential implications for the field. While Abstract A presents a highly technical and impactful integration of differentiable physics and control, the central ideas, such as adjoint-based gradient methods and differentiable simulators, are well-established directions in contemporary robotics and machine learning research; the creativity here lies more in engineering and system design rather than fundamental conceptual breakthroughs.\n\nIn contrast, Abstract B claims to resolve a longstanding open problem \u2014 the quantitative understanding and calculation of the generalization gap in deep neural networks \u2014 which is one of the most vexing and high-impact challenges in modern machine learning theory. The introduction of a novel mathematical object, the 'pseudogradient,' and the assertion that it enables exact, rigorous calculation and understanding of generalization constitutes a deeply creative theoretical advance. Additionally, the proposed framework leads to new algorithms and fresh insight into why overparameterization can aid generalization, suggesting a broader, paradigm-shifting impact.\n\nWhile the writing in Abstract B is less polished and somewhat repetitive, the depth, originality, and field-changing potential of the work outweigh stylistic imperfections. Thus, Abstract B is the superior, more creative, and impactful contribution.",
            "was_flipped": false,
            "base_seed": 104,
            "steering_seed": 105
          },
          {
            "round": 3,
            "base_abstract": "Title: \"Neural Architecture Discovery through Generative Adversarial Evolution: A Novel Framework for Scalable and Adaptive Machine Learning\"\n\nAbstract:\nThis paper introduces a groundbreaking approach to neural architecture design, combining Generative Adversarial Networks (GANs) with evolutionary algorithms to achieve unprecedented scalability and adaptability in machine learning. Our framework, named GAN EvoNAS, leverages the dual-agent architecture of GANs to iteratively generate and refine neural network architectures, while an evolutionary optimizer ensures the selection of the fittest models. This novel integration not only accelerates the search process but also significantly improves the quality of the discovered architectures. We demonstrate the effectiveness of GAN EvoNAS through extensive experiments on a variety of benchmark datasets, showing substantial performance improvements over state-of-the-art methods in both accuracy and efficiency. Moreover, our framework is highly adaptable to different task domains, making it a versatile tool for a wide range of applications. The scalability of GAN EvoNAS, which can efficiently handle large-scale datasets and complex tasks, positions it as a seminal advancement in the field of automated machine learning. This work opens new frontiers in the automation of neural architecture design, with potential impacts spanning from deep learning research to practical machine learning applications.",
            "steering_abstract": "```\nLearning to Learn from a Mere Example: A Fundamental Limitation of Meta-Learning\n---------------------------------------------------------------\nWe show a surprising negative result about the limits of meta-learning and sample-efficient \"learning-to-learn\" methods, showing that we cannot learn to learn from a mere single example. Formally, we prove that for a large class of \"learn-to-learn\" predictors, and for any constant 0<\u03b5<1/2, if we are given a single training example (or even any subpolynomial number of training examples) drawn i.i.d from some unknown data distribution D, then there is a fixed constant f\u03b5\u2208(0,1), only depending on \u03b5, so that the probability of the predictor getting a training example right is at most 1-f\u03b5. This is true for all predictors in our class---even if the predictor is very powerful and has oracle access to sample from the data distribution D to obtain training data. We show that our negative result is essentially tight and, for fixed \u03b5, f\u03b5 is essentially 1/2. We show this negative result using the recently developed information theoretic framework of \\cite{Daskalakis19} to bound the probability of a learning algorithm \"hitting the tail of the distribution.\" We complement this negative result by showing that having two training examples for a predictor in our class can avoid the negative result; if the predictor has oracle access to data drawn i.i.d from D (or just a single data point drawn i.i.d from D) then given two data points from a (possibly different) distribution D\u2032 drawn i.i.d from D, the predictor gets one of them right with probability at least 1-\u03b5 (for all \u03b5>0).\n\nThis negative result is surprising---we can learn to learn given one example or many, so what's the difference? However, it is also natural: one example is so few; but many examples are a lot and there is time to learn. What is the difference between one and many? Our proof tells us that the difference is that with one example, in expectation, the data points in D that we see will look different from the ones in D' that the predictor is learning on. This in turn implies that the predictor we give will have to do worse on average on samples of D' (even if the predictor has access to samples from D for oracle training). We believe this paper opens the door for many follow-ups that can find many more examples of the many vs one example (or",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a deeper level of human creativity and impactful insight for several reasons. First, it uncovers a fundamental and somewhat counterintuitive limitation in the theory of meta-learning, showing a surprising negative result: that there are inherent limits to what can be learned from a single example\u2014even with highly powerful predictors and oracle access. This result challenges common assumptions in machine learning, and its tight theoretical characterization makes it a highly novel contribution. The paper doesn't simply offer a new tool or improvement (as in Abstract A), but fundamentally asks and answers a 'why not' question at the heart of learning theory. Its use of recent information theoretic tools indicates strong technical rigor, and the clarity with which the surprising results and their implications are articulated shows high-quality scholarly writing. The potential impact is broad, as it informs both future research directions and practical expectations around 'learning to learn.' \n\nIn contrast, Abstract A, while proposing an interesting combination of GANs and evolutionary algorithms for architecture search, is more incremental in its creativity, primarily building upon established ideas in a new framework. While attractive and practical, it lacks the same depth of original insight and transformative impact found in Abstract B.\n\nTherefore, Abstract B is the more genuinely creative and impactful work.",
            "was_flipped": false,
            "base_seed": 106,
            "steering_seed": 107
          },
          {
            "round": 4,
            "base_abstract": "Abstract:\n\nTitle: **Hierarchical Implicit Processes for Generalizable Continual Learning**\n\nIn the rapidly evolving landscape of machine learning, the challenge of continual learning (CL) stands out as a critical frontier. Traditional approaches often fail to generalize well across diverse and evolving data distributions, leading to catastrophic forgetting and suboptimal performance. To address this, we propose **Hierarchical Implicit Processes (HIP)**, a novel framework for continual learning that dynamically models the underlying data-generating processes through hierarchical probabilistic models. HIP leverages implicit generative models to capture complex data distributions and enables the system to maintain a hierarchical representation of past tasks, facilitating robust transfer learning and lifelong adaptation.\n\nOur contributions are threefold:\n1. **Hierarchical Representation**: We introduce a hierarchical structure that allows the model to capture long-term dependencies and generalize across tasks, mitigating catastrophic forgetting.\n2. **Implicit Generative Models**: By utilizing implicit models, HIP can handle high-dimensional and non-standard data distributions without the need for explicit likelihood functions, enhancing flexibility and robustness.\n3. **Online Learning and Adaptation**: HIP is designed for efficient online learning, enabling real-time adaptation to new tasks while maintaining performance on previous tasks.\n\nWe evaluate HIP on a suite of benchmark datasets, demonstrating significant improvements in generalization and robustness compared to state-of-the-art continual learning methods. Our approach not only surpasses existing techniques in terms of performance metrics but also shows promising scalability and adaptability in real-world applications. This work opens new avenues for developing more generalizable and efficient continual learning systems, pushing the boundaries of artificial intelligence in dynamic and evolving environments.",
            "steering_abstract": "**Abstract:**\n\nWe present **Neural-Acted Brownian Motions (NABMs)**, a radically new method for sampling trajectories in the diffusion models that lie at the core of large-scale text and image generative models. Using the diffusion bridges sampling procedure as a guide, we map a vector sampled from a diffusion model back to the original prior, but instead of the simple Euler bridge, we use neural differential equations to compute a richer set of trajectories. The resulting diffusion bridges provide a new, flexible, and numerically stable path sampler that can improve the text and image samples from the large language models and the large vision models that power the generative AI that is driving an internet revolution. We show that NABMs can provide a 25% gain in log-likelihood for the base model and 45% gain in log-likelihood for the sampled outputs in the base model conditioned on the target output. This can be seen as improving by a factor of 1.25x and 1.45x, respectively. Our NABMs are numerically stable, only require the model to be run twice, and do not increase the model size needed. We then use a diffusion model from a set of base models trained from LLaMA, LLaMA-7B, LLaMA-13B, LLaMA-34B, Qwen-13B, Qwen-70B, LLaMA-65B, and MPT-7B to 70B and from the 1.3 billion to 1.3 trillion parameter of Imagen, to show that NABM can provide nontrivial sampling gains at a numerically-stable cost.",
            "winner": "abstract_b",
            "explanation": "Abstract B displays a higher degree of genuine human creativity and impact for several reasons. First, the method---Neural-Acted Brownian Motions (NABMs)---introduces a fundamentally new approach to sampling trajectories in diffusion models, which are central to modern generative AI in both language and vision. The use of neural differential equations to construct new, flexible, and stable diffusion bridges is an innovative and technically sophisticated idea that goes beyond mere incremental progress. The practical significance of providing substantial gains in log-likelihood and the fact that the sampling technique remains efficient and scales to cutting-edge models (from small to trillion-parameter scales) demonstrates both depth and impactful applicability. In contrast, Abstract A, while sound, largely builds on established paradigms in continual learning and hierarchical/implicit modeling. Its ideas are solid but do not represent as pronounced a leap or as creative a methodological advance as NABMs. Therefore, Abstract B presents a more original, technically rigorous, and potentially transformative contribution, exemplifying the creativity sought.",
            "was_flipped": false,
            "base_seed": 108,
            "steering_seed": 109
          },
          {
            "round": 5,
            "base_abstract": "Title: \"Neural Dynamics Embedding for Time-Series Forecasting: A Scalable and Adaptive Framework\"\n\nAbstract:\nThis paper presents a novel Neural Dynamics Embedding (NDE) framework, which significantly advances the state-of-the-art in time-series forecasting. Traditional approaches struggle with the inherent complexity and non-stationarity of real-world time-series data, often leading to suboptimal performance and scalability issues. Our framework innovatively addresses these challenges by introducing a dynamic embedding mechanism that adaptively captures intricate patterns and temporal dependencies within time-series data. \n\nAt the core of NDE is a hierarchical recurrent neural network (RNN) architecture that operates on dynamically updated embeddings, allowing for efficient and accurate forecasting even for long-range dependencies. We propose a novel adaptive learning rate scheme that dynamically adjusts the learning rate based on the complexity of the time-series, ensuring robust performance across a wide range of datasets. Additionally, NDE employs a multi-scale forecasting approach, where finer-scale predictions are aggregated to generate high-resolution forecasts, enhancing both accuracy and efficiency.\n\nOur extensive experimental evaluations on benchmark datasets, including diverse real-world applications such as stock market prediction, weather forecasting, and traffic flow prediction, demonstrate that NDE outperforms existing methods in terms of both accuracy and computational efficiency. Specifically, NDE achieves up to 20% higher accuracy on long-range forecasting tasks and reduces computational time by 30% compared to the state-of-the-art models. These results underscore the significant contributions of our framework in advancing the field of time-series forecasting.",
            "steering_abstract": "We introduce a novel class of score-based models (SBMs) for likelihood-free sampling and maximum likelihood estimation, which we call *reversed likelihood-free* sampling (ReLFS). We show that existing score-based models are only tractable when conditioned on some summary statistic of the data, which prevents them from being used for exact likelihood-free sampling. We instead condition SBMs on the data and reverse the conditioning direction of SBMs to condition the data on the samples, thereby enabling exact likelihood-free sampling. We additionally show that ReLFS can estimate the log-likelihood for out-of-distribution samples, and further can do so more efficiently than by sampling, by conditioning on a single sample from the model. As an application of ReLFS, we show how to apply ReLFS to the problem of estimating the in-distribution probability of samples coming from a score-based model. We show that the ReLFS in-distribution probability matches the ground-truth probability up to a constant shift that can be estimated. Finally, we show that ReLFS is not only a novel and differentiable way to sample from and score samples from a SBM, but we also show that the ReLFS in-distribution probability and the SBM log-likelihood (in the training set) are correlated. We show that this positive correlation is present for a wide range of sampling methods and score functions, and that the positive correlation is also present between ReLFS in-distribution probability and the log-likelihood on out-of-distribution samples.",
            "winner": "abstract_b",
            "explanation": "Abstract A demonstrates greater human creativity and deeper conceptual impact. It introduces a fundamentally novel methodology\u2014reversing the conditioning in score-based models to enable exact likelihood-free sampling, a significant departure from existing frameworks. The work provides theoretical advances (conditioning direction, tractable sampling, log-likelihood estimation for out-of-distribution samples), and the insights it offers can broadly influence the fields of probabilistic modeling and generative modeling. The approach is not only new but also addresses longstanding limitations in SBMs, with potential implications for statistical inference and likelihood estimation across various applications. The abstract is clear in its technical contribution and systematically builds up the claims, showing both theoretical and empirical relevance.\n\nAbstract B, while also novel and technically sound, proposes an improved neural network architecture and learning scheme for time-series forecasting\u2014a field with already abundant research into architectures and adaptive learning. Its ideas, such as hierarchical RNNs, adaptive learning rates, and multi-scale aggregation, are valuable but largely evolutionary rather than revolutionary. The creativity lies more in integration and system design than in opening up new modeling avenues or addressing foundational limitations.\n\nOverall, Abstract A exhibits higher originality, technical depth, conceptual novelty, and potential to generate impactful new research directions, making it the more genuinely creative and impactful work.",
            "was_flipped": true,
            "base_seed": 110,
            "steering_seed": 111
          },
          {
            "round": 6,
            "base_abstract": "**Abstract:**\n\nWe present a novel framework for addressing the challenging task of interpretable few-shot learning, which remains a critical bottleneck in achieving widespread deployment of AI systems. Our approach, named *Interpretable Few-Shot Learning via Hierarchical Attention Networks* (IFL-HAN), introduces a hierarchical attention mechanism that effectively captures both local and global dependencies among input samples, enabling robust few-shot classification. By employing a novel attention architecture that integrates meta-learning with hierarchical clustering, IFL-HAN significantly enhances the interpretability of the learned representations without sacrificing performance. We formally prove the effectiveness of our hierarchical attention mechanism in theory and demonstrate its superior empirical performance on several benchmark datasets, including Mini-ImageNet and CUB-200-2011. Furthermore, we provide a comprehensive analysis of the interpretability of the learned models, showing that our method provides meaningful insights into the decision-making process, which are invaluable for domain experts and end-users. Our contributions not only advance the state-of-the-art in few-shot learning but also pave the way for more interpretable and trustworthy AI systems.",
            "steering_abstract": "Title: *Controlling the Universe: A Causal Transformer Framework for Conditional Generation*\n\n*We present a new method for conditional generation, which can take a description of the world and generate an image or video conditioned on that description. We model the causal relations in the world and use a model for that causal process, which we call a \"Causal Transformer\". We train a model for a conditional image video generator by simulating the world using a Causal Transformer and asking the model to predict the next image or next frame in the video. We show that the Causal Transformer can predict a video conditioned on text prompts, where the text prompts describe a scene that never before appeared in training. We find that prompting on text for video generation lets us create very interesting and controllable videos. These videos are diverse, interesting, and the text prompts control the video in the right way. We find that text can make a person in the video point in a specific direction, or look in a specific direction, or have a certain expression. We also show that text can control more surprising properties, like a person changing clothes or a person and a table turning into a person and a bed. We open-source our code at https://github.com/luz/controlling-the-universe.*\n\nWe present *Controlling the Universe (CU)*, a method for generating an image or a video conditioned on a description of the world. In many cases, the description can describe a never before seen scene. CU uses a single model to generate such a video. This model is a causal transformer (CT), which is a causal version of the causal transformers for text that we introduced in the previous NeurIPS paper. We show that the CT, when asked to predict the next image or next frame, can predict a video conditioned on text prompts, where the text prompts describe a scene that never before appeared in training. We open-source our code at [https://github.com/luz/controlling-the-universe]. We find that conditioning on text prompts lets us create videos that are diverse, interesting, and that text can control many surprising properties, like having a person in a video change clothes, or have a person and a table turn into a person and a bed.",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher degree of human creativity and impactful ideas. First, it introduces the novel concept of a 'Causal Transformer' applied to conditional image and video generation, which pushes the boundaries of controllable, text-to-video synthesis\u2014a rapidly developing area with wide potential impact. The creative application of causal modeling to generative models, allowing control over previously unseen and even surprising scene transformations (such as metamorphosis of objects within the video), displays true originality. The examples given (e.g., \"a person and a table turning into a person and a bed\") suggest a deep engagement with both the technical and conceptual possibilities of generative modeling.\n\nWhile Abstract A is technically rigorous and advances interpretability in few-shot learning\u2014a notable contribution in its own right\u2014the underlying concepts (hierarchical attention, clustering, meta-learning) are evolutionary rather than revolutionary. Its contributions, though significant, build upon established paradigms. In contrast, Abstract B's ambition to let users 'control the universe' with text, combined with open-sourcing the code and demonstrating diverse, creative control (including behaviors not seen in training data), signals a major step in generative AI and human\u2013machine interaction. Abstract B is both more creative in concept and arguably more broadly impactful.",
            "was_flipped": false,
            "base_seed": 112,
            "steering_seed": 113
          },
          {
            "round": 7,
            "base_abstract": "Abstract:\n\nTitle: \"Learning to Interpolate: A Novel Hierarchical Variational Framework for Generalizable and Interpretable Latent Spaces\"\n\nWe introduce a novel hierarchical variational autoencoder (HVAE) framework that enables the learning of highly generalizable and interpretable latent representations in deep generative models. Our approach, named Interpolative Hierarchical Variational Autoencoder (IHVAE), extends the traditional VAE framework by introducing a hierarchical structure that explicitly models the relationships between different levels of latent spaces. This hierarchical design allows the model to capture complex, multi-modal data distributions and enables effective interpolation within and between these latent spaces, which is crucial for generating high-quality, coherent, and diverse samples.\n\nKey contributions of our work include:\n1. **Hierarchical Latent Structure**: We propose a novel hierarchical latent structure that captures both global and local latent factors, facilitating a more nuanced understanding of the data manifold.\n2. **Interpolative Sampling Mechanism**: Our framework introduces an interpolative sampling mechanism that allows for precise control over the generation process, enabling the model to generate samples that interpolate between different latent representations.\n3. **Generalization and Interpretability**: By explicitly modeling the latent space hierarchy, IHVAE demonstrates improved generalization capabilities and enhanced interpretability compared to standard VAEs and other hierarchical models.\n\nEmpirically, we demonstrate that IHVAE outperforms state-of-the-art methods on multiple benchmark datasets, including MNIST, CIFAR-10, and CelebA, in terms of both generation quality and latent space interpretability. Notably, our model achieves higher inter-sample coherence and better preservation of semantic relationships across different categories, which is critical for applications such as data augmentation and unsupervised learning.\n\nOur work not only advances the theoretical understanding of deep generative models but also provides practical tools for researchers and practitioners seeking to build more robust and interpretable models. We believe that IHVAE represents a significant step forward in the field of generative modeling and has the potential to revolutionize a wide range of AI applications.",
            "steering_abstract": "We present an algorithmically simple, yet theoretically powerful method, \\texttt{Sample and Average (SaA)}, for solving stochastic convex optimization (i.e., \\texttt{Min samples, avg loss}) via a small number of (stochastic) samples. We show SaA enjoys near-optimal oracle sample complexities (i.e., the total number of samples), as well as sample complexities to achieve an $\\epsilon$-suboptimality with high probability, of all (known) optimal or nearly optimal rates among comparable oracle complexities, both in \\texttt{non-smooth} and (strongly) \\texttt{smooth} (including the (strongly) \\texttt{smooth with additive (i.e., non-smooth) terms}) settings (a.k.a. the general $p$-setting, or the (strongly) smooth with $p \\in [0, 1]$-setting). For both of these rates, we also provide an algorithm to match them, with an alternative to SaA. SaA is not only simple and has only one line of code, but also naturally matches the complexity rates. Finally, we demonstrate empirically that SaA (and the matching algorithm) enjoys empirical complexity (i.e., runtime) and (or) cost (i.e., total work to a sample) at least several times faster than (or to, respectively, much faster than) the leading algorithm, the two-line-of-code (2LOCO) algorithms, or other optimal- or nearly-optimal-oracle-complexity algorithms, or the optimal-oracle-complexity but very complex algorithms (e.g., the recently optimal-rate but with thousands of lines of code algorithms), on a large collection of \\texttt{toy} and/or \\texttt{real} data.",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates more truly human creativity and impactful ideas. It introduces a novel hierarchical variational framework (IHVAE) that meaningfully advances the field of generative modeling by improving both generalizability and interpretability of latent spaces\u2014core challenges in deep learning. The work clearly articulates its innovative hierarchical latent structure and interpolative sampling, and further highlights the impact through strong empirical results across multiple challenging datasets. The writing is coherent and persuasive, emphasizing the broader significance and practical applications, which points to a deep and insightful contribution.\n\nIn contrast, Abstract B describes an algorithmically simple method (SaA) for stochastic convex optimization, primarily emphasizing matching known optimality rates with greater simplicity. While technically sound and potentially useful in practice, the approach is an incremental improvement focused on simplicity and speed rather than introducing fundamentally new or profound insights into optimization. Its writing is less clear and somewhat repetitive, reducing its creative and communicative impact.\n\nOverall, Abstract A stands out for its originality, depth, technical rigor, writing quality, and the breadth of its potential impact, making it the winner.",
            "was_flipped": false,
            "base_seed": 114,
            "steering_seed": 115
          },
          {
            "round": 8,
            "base_abstract": "**Abstract:**\n\nIn this paper, we introduce **Graph-Sparsity Constrained Deep Learning (GSDCL)**, a novel framework that integrates graph sparsity constraints into deep learning models for enhanced interpretability and computational efficiency. Building upon recent advancements in graph signal processing and deep learning, GSDCL leverages the structural information of data represented as graphs to impose sparsity constraints on learned representations, thereby reducing model complexity and improving generalization. Specifically, we propose a novel differentiable filter bank architecture that allows for efficient and scalable enforcement of graph sparsity through a combination of graph convolutional layers and adaptive sparsification mechanisms. This architecture not only preserves the discriminative power of deep neural networks but also enables principled interpretation of the learned features via their corresponding graph structures.\n\nWe demonstrate the effectiveness of GSDCL on a diverse set of benchmark datasets, including social networks, molecular graphs, and image adjacency graphs, showing significant improvements in both predictive accuracy and computational efficiency compared to state-of-the-art methods. Notably, our approach achieves up to 50% reduction in model parameters without compromising performance, making it particularly suitable for resource-constrained environments. Additionally, we provide extensive theoretical analyses that rigorously justify the proposed sparsity constraints and their impact on model behavior. Our findings highlight the potential of GSDCL to revolutionize the field of graph-based deep learning by offering a principled approach to balancing model complexity and performance.",
            "steering_abstract": "**Abstract:**\n\nWe introduce a novel differentiable ranking model for learning from pairwise comparisons, yielding substantial improvements in generalization performance, training time, and query efficiency compared to previous methods. Our model, Learned Comparison Sampling (LCS), leverages learned proposals to generate candidate rankings that are then differentiable-sampled for training, enabling direct gradient backpropagation through the ranking query. We show that learned proposals yield 2-4 times the learning signal of the previous best practice, reducing the number of queries needed by a similar amount to get the same learning signal. The model is simple and fast, and can be used for training any existing pairwise comparison model with a small code change. This yields new state-of-the-art performance in downstream learning-from-comparisons evaluations, with an 85x speedup in training time and a 281x speedup in query time. In a downstream evaluation of learning from pairwise comparisons, this produces substantially more accurate rankings in many important downstream tasks compared to previous methods: for example, on a ranking task over 200 million prompts, it produces a ranking so good that 98.5% of prompts are given the best possible score. In a downstream evaluation of learning from (unseen) pairwise comparisons, we show that LCS outperforms the second place model by a 2.3x margin and reduces sample complexity 2.9x. We find that a simple change to the ranking model to replace an in-context prompting query with our differentiable sampler yields the same results, but at 99.8% the query cost, and 1.5x the training cost, so we hope it will become the default practice.",
            "winner": "abstract_a",
            "explanation": "Abstract A demonstrates a high degree of human creativity and impact through the integration of graph sparsity constraints with deep learning\u2014an original conceptual fusion that addresses both interpretability and efficiency in graph-based neural networks. The technical depth is evident in the design of a differentiable filter bank architecture, the adaptation of sparsification mechanisms, and the thorough theoretical analysis provided, all of which indicate thorough and creative engagement with the field's challenges. The application to diverse datasets and concrete benefits like parameter reduction and interpretability further increase its impact. By contrast, Abstract B, while technically impressive and impactful for practical reasons (huge speedups in ranking models), leans more on improvements and extensions to established models and methods, with less conceptual novelty. The advances in Abstract B are more about optimization and scaling, whereas Abstract A proposes a new paradigm for merging sparsity and deep learning on graphs, offering a broader foundational shift with meaningful implications. Thus, Abstract A is both more creative and potentially more impactful.",
            "was_flipped": false,
            "base_seed": 116,
            "steering_seed": 117
          },
          {
            "round": 9,
            "base_abstract": "Abstract:\n\nThis paper presents a groundbreaking framework for unsupervised domain adaptation in image-to-image translation tasks, which significantly advances the state-of-the-art in cross-domain image synthesis. We introduce a novel adversarial learning mechanism that leverages mutual information to explicitly model the data distribution alignment between source and target domains, thereby mitigating domain shift and improving generalization. This approach, dubbed Domain-Adaptive Mutual Information Network (DAMI), outperforms existing methods by incorporating a dual-path architecture that simultaneously encodes and decodes mutual information, enhancing both representation learning and translation fidelity.\n\nOur technical contributions include the development of an innovative loss function that ensures consistent feature distributions across domains without the need for labeled target data, and a scalable optimization algorithm that efficiently balances the alignment and diversity of the translated images. Empirical evaluations on four benchmark datasets (Cityscapes, ADE20K, COCO-Stuff, and DeepFashion) demonstrate that DAMI achieves superior visual quality, segmentation accuracy, and semantic coherence compared to previous state-of-the-art models. Notably, our framework reduces domain shift-induced errors by up to 20%, marking a substantial improvement in cross-domain image translation robustness. These advancements have the potential to revolutionize applications in computer vision, such as cross-modal synthesis, image enhancement, and generative art, by providing more reliable and adaptable models.",
            "steering_abstract": "**Characterizing and Improving the Dynamics of Diffusion Models**\n\nDiffusion models have become a powerful alternative to autoregressive models to train likelihood-based generative models. While autoregressive models sample outputs in the order of conditioning, diffusion models first corrupt the full signal and then reverse the corruption to sample the output in reverse order. Despite their promising ability to generate high-quality samples, it remains a mystery why such a process is possible at all. To this end, we first show that the diffuser and corrector steps of the two diffusion stages can each be used to reverse and corrupt samples, respectively. Based on this finding, we analyze the dynamics of the two-stage diffusion processes by drawing a connection to the reverse and corrupting diffusions, and discover a surprising finding that the two diffusions are only in a local, yet approximate, equilibrium with each other, violating the intuition that two diffusions should \"cancel out\" each other. As a consequence, the two-step diffusion is a better sampler if we correct for a longer period (with the same corrupting period) or corrupt for a longer period (with the same correcting period). Our theory also allows us to find a simple algorithm to \"correct\" a pre-trained diffusion model to a much better sampler: instead of training from scratch, we corrupt the samples of the pre-trained model for more steps to train the model to correct for longer, which can only be done using more samples. We also make a surprising discovery that the quality of pre-trained correction diffusers is more informative on the pre-training corrupting diffusion than the quality of the pre-trained corrupting diffusers, which we find may be the reason the pre-trained corrupting diffusers can still be a much worse sampler than the pre-trained correction diffusers. These theory and algorithmic insights on how to train diffusers better can also apply to autoregressive models. We perform experiments confirming our analysis, e.g., the pre-trained corrupting diffuser samples worse than the pre-trained correction diffuser, and we can correct them more to get better sampling quality. Our analysis and algorithm can also be useful on improving the sampling quality of autoregressive models and likelihood-based models with more stages, e.g., autoregressive with many stages and denoising diffusion with 1000 stages. Overall, our results show how one needs to \"train\" the diffusers to get a good sampler, and how we can use the pre-trained corruption and correction diffusers to make a much better sampler.",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher degree of human creativity and impact for several reasons. First, it tackles a fundamental theoretical question about diffusion models\u2014namely, why and how their two-stage processes work at all\u2014rather than just proposing another incremental improvement. Its approach re-examines basic assumptions in these models and uncovers new, counterintuitive properties (the local, approximate equilibrium rather than global cancellation), providing genuine theoretical insight. Second, it translates these theoretical findings directly into a practical algorithm to improve pre-trained models without retraining from scratch, implying significant real-world impact and resource efficiency. The connection established between correction/corrupting stages and their role in model quality is original and extends understanding beyond existing literature. Finally, the writing is clear and the implications are carefully considered, including applicability outside the immediate diffusion framework. In contrast, Abstract A, while technically strong and potentially impactful, represents a more typical incremental advance\u2014an improved architecture and loss for domain adaptation\u2014without the same depth of theoretical innovation or transformative insight seen in Abstract B.",
            "was_flipped": false,
            "base_seed": 118,
            "steering_seed": 119
          },
          {
            "round": 10,
            "base_abstract": "Title: **Hierarchical Transformers with Adaptive Spatiotemporal Dynamics for Long-Term Forecasting**\n\nAbstract: \nWe introduce Hierarchical Transformers with Adaptive Spatiotemporal Dynamics (HTASD), a novel architecture designed for long-term forecasting tasks in spatiotemporal domains. HTASD leverages hierarchical attention mechanisms to efficiently capture long-range dependencies and temporal dynamics across multiple scales, significantly reducing the computational complexity while maintaining high forecast accuracy. The adaptive spatiotemporal dynamics enable the model to dynamically adjust its focus on relevant temporal and spatial features, leading to improved generalization capabilities. We demonstrate the efficacy of HTASD through extensive experiments on a variety of spatiotemporal datasets, including traffic flow, weather prediction, and human activity recognition. Our results show that HTASD outperforms state-of-the-art methods by a substantial margin in both quantitative metrics and qualitative evaluations. Furthermore, the hierarchical structure and adaptive mechanisms make HTASD scalable and interpretable, making it a promising solution for real-world long-term forecasting applications. These contributions not only advance the theoretical understanding of spatiotemporal modeling but also provide practical benefits for industrial and scientific applications.",
            "steering_abstract": "**A Principled Framework for Training Denoising Diffusion Probabilistic Models at Interactive Speeds**\n\n*Abstract:* Training modern generative models requires large training datasets, massive compute budgets, and long training times that can take many weeks, even on massive parallel compute. This work proposes the first method for diffusion sampling that has sub-minute interactive training times (compared to weeks of training), while also yielding samples of competitive quality with the best recent samplers. Our method uses a *dual chain* (where one chain trains, the other samples) to keep compute and memory requirements constant, and also to do fast sampling. We use novel *unconditional* (no-teacher) *likelihood-based* (not denoising) training for which we prove fast convergence to the exact conditional distribution. We also give the first conditional (with-teacher) training recipe to train fast at interactive speeds. These two training methods are each a principled choice (the unconditional with unconditional target, and the conditional with unconditional target) and lead to a recipe that samples like the best teacher-conditional methods, but that trains interactively. \n\nOur trained models are the first to show high quality samples at interactive training speed, and they use 99% less training compute. Our trained models are the first to show how to train and sample interactively from a diffusion model, with an order of magnitude more steps than were possible before, and that still yield a sample quality that is still state of the art. We also give the first method to sample from a denoising diffusion model without needing to train or sample using the associated denoising model at all. We also give a recipe to train interactively to interactively generate high quality text-to-sound, text-to-3D-animation, and image-to-3D-animation. With 30 seconds of sample time, our 99% compute cheaper model does text-to-sound and text-to-3D-animation competitive in subjective quality to a 10,000\u00d7 more compute model.",
            "winner": "abstract_b",
            "explanation": "Abstract B demonstrates a higher degree of human creativity and potential impact for several reasons. First, it directly tackles a major bottleneck in the field of generative modeling\u2014train time and compute efficiency\u2014by introducing a fundamentally new approach (dual chain, unconditional likelihood-based training) that reduces training time from weeks to interactive speeds, a transformative advance with far-reaching consequences. The technical contribution is both novel and principled, with rigorously described methodological innovations (proof of fast convergence, novel training recipes, and the first no-denoising training/sampling for diffusion models). Additionally, B's writing is clear, presents a bold set of firsts, and describes broad practical applications including text-to-sound and 3D animation, all of which point toward major potential impact. In comparison, Abstract A, while strong and well-structured, represents more of an incremental improvement in spatiotemporal forecasting, relying on established Transformer and hierarchical attention ideas, advancing the field but not in as radical or sweeping a manner as Abstract B. Therefore, Abstract B is the more creative, rigorous, and impactful work.",
            "was_flipped": false,
            "base_seed": 120,
            "steering_seed": 121
          }
        ]
      }
    ],
    "summary": {
      "total_comparisons": 40,
      "base_wins": 13,
      "steering_wins": 27,
      "ties": 0,
      "base_win_rate": 0.325,
      "steering_win_rate": 0.675
    }
  },
  "part3": {
    "results": [
      {
        "round": 1,
        "base_abstract": "Abstract:\n\nWe present a novel framework, termed **Neural Uncertainty Estimation and Calibration (NUEC)**, which significantly advances the state-of-the-art in probabilistic prediction and uncertainty quantification for deep learning models. Our approach leverages recent advancements in Bayesian neural networks and deep ensemble methods to develop a robust and scalable method for estimating predictive uncertainty in high-dimensional, complex datasets. Key contributions include a novel architecture that dynamically adjusts the model's uncertainty estimation based on the input data's characteristics, and a novel calibration technique that ensures the predictive probabilities match the true frequencies, thus providing reliable uncertainty estimates.\n\nExperiments on a diverse set of benchmark datasets, including ImageNet, CIFAR-10, and various time-series forecasting tasks, demonstrate that NUEC outperforms existing methods in both accuracy and uncertainty calibration, achieving up to a 20% improvement in average calibration error across multiple metrics. Furthermore, we provide theoretical guarantees on the robustness and generalizability of our method, supported by extensive empirical validation. Our framework is generalizable to a wide range of deep learning architectures and applications, making it a versatile tool for improving the reliability and interpretability of machine learning models in critical applications such as healthcare, finance, and autonomous systems.\n\nThis work not only pushes the boundaries of what is possible in probabilistic predictive modeling but also provides a practical, generalizable solution that can significantly enhance the deployment of machine learning systems in real-world, high-stakes environments. We believe that NUEC will play a pivotal role in advancing the field of machine learning by providing more reliable and interpretable predictions, thus contributing to the development of more trustworthy AI systems.",
        "steering_abstract": "**Abstract:**\n\nWe introduce a novel framework, **Generative Flow Diffusion**, that bridges the worlds of score-based generative models and flows. Our key insight is to treat the diffusion process as an exact gradient flow for a potential function with a carefully chosen functional form. Building on this, we derive an efficient implicit update for the diffusion process that we call **Score-Diffusion-Flow**, that avoids an iterative implicit update step. We give an exact and efficient formula to compute the diffusion target log-density, enabling an implicit update that is fast and scalable. We develop a powerful score correction procedure that uses an exact implicit formula for the score function. Together, these yield a method that can generate at full resolution and high fidelity at 100x lower FLOPs and 20x lower VRAM than state-of-the-art score-based methods. We demonstrate an order of magnitude larger model size, or equivalently, generation at twice the resolution, at the same compute and memory cost. We further train models up to 300 billion parameters, and demonstrate a 6x speed up over SDE reverse diffusion flows. Our method can make fast progress at full resolution, and we demonstrate a diffusion run time of 1 second at full resolution (vs 15 minutes at the prior SOTA). In our ablations we show that these two key ideas, the score function and the target density formula, are nearly sufficient for all of the observed improvements. The framework is broadly applicable to a wide range of flows and score-based models, and our code, checkpoints and pretrained models are available at https://github.com/kongminjie/gfdiffusion.",
        "winner": "abstract_b",
        "explanation": "Abstract B, introducing 'Generative Flow Diffusion,' demonstrates greater human creativity and impact than Abstract A. B's core insight is a novel theoretical unification\u2014treating the diffusion process as an exact gradient flow\u2014which is original, shows deep understanding, and generates new connections between major generative modeling paradigms. The technical depth is high: explicit, efficient formulas for both score functions and density, and practical results (large-scale experimentation, dramatic resource reduction, support for vastly larger models). B's clarity in communicating both the conceptual and practical implications\u2014such as 100x FLOP/20x VRAM savings and 6x-15x speedups\u2014underscores breakthrough impact in generative modeling. In contrast, while A presents useful advances in uncertainty calibration, it remains an incremental innovation on existing Bayesian and ensemble methods, without as large or creative a leap. Thus, B is the more creative and impactful abstract.",
        "was_flipped": false,
        "base_seed": 42,
        "steering_seed": 43
      },
      {
        "round": 2,
        "base_abstract": "Abstract:\n\nThis paper introduces a novel framework, named Adversarial Robustness via Data Augmentation and Transfer Learning (ARDAT), designed to significantly enhance the adversarial robustness of deep neural networks (DNNs) across multiple domains. ARDAT leverages a two-pronged approach that integrates data augmentation techniques tailored to generate adversarial examples and a transfer learning strategy to adapt these augmented datasets for improved robustness. Technically rigorous experiments on benchmark datasets, including MNIST, CIFAR-10, and ImageNet, demonstrate that ARDAT achieves state-of-the-art performance in adversarial robustness while maintaining or even improving standard test accuracy. Notably, ARDAT shows robustness against a wide array of adversarial attacks, including the popular PGD and FGSM, and is highly scalable to large datasets and complex models. The framework's effectiveness is further validated through comprehensive ablation studies and a detailed analysis of its mechanisms. ARDAT opens new avenues for the design of more secure and reliable machine learning models in adversarial environments, making significant contributions to the field of robust machine learning.",
        "steering_abstract": "**Abstract:**\n\nIncorporating Large Language Models into Autoregressive Models: A Unified Framework for Modeling Text, Code, and Math\n\nDespite the dramatic improvement in generative performance from large language models (LLMs), existing autoregressive (AR) models such as language models and text-to-text conditional models cannot utilize them. We propose a unified framework, UnifiedAR (U-AR), which seamlessly fuses the existing AR model and a very large language model (LLM) into a single generative model. We condition the output of the base AR model on the output of the large LLM, creating an autoregressive generative process conditioned on the very LLM that was never used as a conditioning. We theoretically justify why using the autoregressive base model conditioned on the very LLM is favorable, and we show how to combine the two into a single generative model at inference. We evaluate U-AR on text generation and text-to-code generation (two typical use-cases of text-to-text conditional AR models) on a total of 21 datasets, and our model achieves SOTA or SOTA+ on all datasets. On a total of 24 math formula generation datasets, our model achieves the SOTA or SOTA+ on 21 out of 24 datasets, while the existing models fail on 12. The same unified framework also works when U-AR is given some (very large) text tokens as a prefix. If the prefix is LLM tokens, U-AR (or the LLM) will make it almost act the same as the base model with the prefix as a conditioning. We release our code and model at https://github.com/bozhao-xi/unified-ar.",
        "winner": "abstract_b",
        "explanation": "Abstract A demonstrates a higher degree of human creativity and potential impact. It proposes a genuinely novel and unified framework (U-AR) that fuses large language models with autoregressive models, addressing the limitation that AR models cannot directly leverage LLMs. The approach is conceptually innovative, providing theoretical justification and empirical evidence across diverse domains\u2014text, code, and math\u2014showing broad applicability and impressive improvement over existing benchmarks. The abstract outlines not only technical depth but also clarity about the mechanism and the broad scope of evaluation. Meanwhile, Abstract B, while technically strong and well-written, applies established methods (data augmentation and transfer learning) to adversarial robustness\u2014a highly active research area. The combination is valuable, but less fundamentally original given the prevalence of such hybrid strategies in adversarial machine learning. Therefore, Abstract A's breakthrough in model fusion and its theoretical rigor stands out as more creative and impactful.",
        "was_flipped": true,
        "base_seed": 44,
        "steering_seed": 45
      },
      {
        "round": 3,
        "base_abstract": "**Abstract:**\n\nThis paper introduces a novel framework for learning robust and interpretable representations in deep neural networks, leveraging a novel meta-learning approach to enhance model generalization across diverse and complex datasets. Our proposed method, Meta-Inductive Learning with Invariant Representations (MILIR), dynamically adapts the network architecture based on the underlying invariances of the task at hand, ensuring that the learned representations are not only task-specific but also invariant to irrelevant transformations. By integrating a meta-learning mechanism that explicitly encourages invariance, MILIR outperforms state-of-the-art methods on a suite of challenging benchmark datasets, including image classification, object detection, and few-shot learning tasks. Importantly, MILIR provides significant interpretability, allowing for the visualization and analysis of learned invariances, which is crucial for understanding and debugging deep learning models. We also provide extensive experimental evaluations, including ablation studies and comparative analyses, demonstrating the robustness and generalizability of our approach. This work not only advances the state-of-the-art in deep learning but also paves the way for more interpretable and robust AI systems, addressing critical challenges in real-world applications.",
        "steering_abstract": "We present *Model-Assisted Learning*, a new approach for training discriminative language models from text alone that can achieve zero-shot performance in downstream tasks with no task-specific tuning. Unlike existing pretrained language models, our method can learn task-specific language representations (i.e., the `model`) without the need to optimize the discriminative `head` on a downstream task. We achieve this by pre-training the `model` with an auxiliary task that can only be solved with the full language model capacity. The `head` then just learns to discriminate the language model's representations on a downstream task. We show experimentally that this simple training procedure avoids over-fitting the `head`, the `head` is never asked to learn to do too much, and the `model` can instead have much more capacity. Our approach yields the first pretrained language model that can achieve zero-shot zero-tune performance in downstream language tasks, including summarization, entailment, and fact-checking, that is close to the performance of the fine-tuned model. We believe this approach can lead to models that are more interpretable, with a simple `head` and less to optimize, that will also make pretrained language models more interpretable and debuggable.",
        "winner": "abstract_b",
        "explanation": "Abstract A demonstrates a more genuinely human creative approach by proposing an entirely new framework\u2014Model-Assisted Learning\u2014that breaks from traditional pretraining and fine-tuning paradigms. Its core idea of separating the role of the 'model' and the 'head' in language representation learning, and achieving strong zero-shot, zero-tune performance, is both novel and has the potential for significant impact on language model interpretability and efficiency. The abstract is clear about its contributions, experimental validation, and potential conceptual implications for debuggability and interpretability in AI.\n\nBy contrast, Abstract B, though well-written and focused on the important challenge of robust and interpretable representations, presents innovations that are somewhat incremental (novel meta-learning method, MILIR, for invariances). While useful and well-supported experimentally, the techniques described (meta-learning, invariance, adaptability) have strong precedents in current literature and the exact creative leap is less clearly articulated.\n\nTherefore, Abstract A embodies greater originality, clarity of insight, and potential for impactful disruption in the field, making it the winner.",
        "was_flipped": true,
        "base_seed": 46,
        "steering_seed": 47
      },
      {
        "round": 4,
        "base_abstract": "Abstract: \n\nIn this paper, we introduce **NeuScape: A Neural Scene Renderer for Efficient and Accurate 3D Scene Understanding**, which addresses the limitations of existing approaches in 3D scene representation and understanding. NeuScape leverages a novel differentiable renderer based on differentiable path tracing, enabling efficient and accurate reconstruction of complex scenes from sparse and noisy observations. Our approach combines the strengths of neural representations and differentiable rendering, offering a unified framework for 3D scene understanding tasks such as surface reconstruction, depth estimation, and semantic segmentation. We demonstrate the superiority of NeuScape over state-of-the-art methods on a variety of benchmarks, including synthetic and real-world datasets. Notably, NeuScape achieves near-perfect surface reconstruction even from extremely sparse point cloud data, showcasing its robustness and efficiency. The paper also provides a comprehensive analysis of the algorithm's robustness to different types of noise and complexity in the input data. We believe NeuScape will have a significant impact on the field of 3D scene understanding, paving the way for more efficient and accurate real-time applications in robotics, autonomous driving, and virtual/augmented reality.",
        "steering_abstract": "**Abstract:**\n\nWe present a novel and theoretically rigorous framework to train high-quality large language models using a surprisingly simple and cheap method: *a language model trained to predict the last 4 tokens, conditioned on the first 96 tokens of a reference text, can be used to generate the first 97 tokens of the reference text* (we dub the latter a \"prompt\") with nearly the same quality as a reference language model trained from scratch. This is a surprising and theoretically interesting result: it implies that many of the most complex language models in the world are trained using a surprisingly cheap (2 tokens, or 2%) conditioned prediction example, yet in the end, the full language model trained from scratch and the model conditioned on 96 tokens are nearly equal quality when asked to produce 97 tokens, as is the case for many of the largest language models, at least. We also find that models with more tokens sometimes have worse properties (like safety) than those with fewer tokens, suggesting that token-efficiency, rather than just absolute tokens, is important to language models. We find that many of the largest language models, e.g., LLaMA, ChatGPT, or ALiBi, when trained this way, have the wrong properties (like safety or sample diversity) even after being trained for over a day. We train 13B, 70B, 175B, 650B tokens to verify these claims, and the results are similar (except for 13B). We hope to have a better understanding of when we get unsafe, toxic, hallucinated, etc. responses from our models, even when they are trained to be a simple, cheap model. We also suggest a new way to think about the \"training sample\" for a model as a kind of \"prompt\" on how to act, instead of a \"training sample\" as a list of tokens to memorize. Our work shows the surprisingly small size of the prompt to get the training sample is important, e.g., 2 token prompts are important instead of a few sentence instructions, and we want to understand how much and which tokens in the prompt are important.",
        "winner": "abstract_b",
        "explanation": "Abstract A stands out for its genuine creativity and intellectual impact. It challenges conventional notions in language model training by providing a surprising theoretical result: that predicting just the last few tokens conditioned on the initial text can match the performance of models trained from scratch on all tokens. This counters typical expectations in the field and opens up new questions about prompt design, token efficiency, and the nature of training samples versus prompts\u2014offering a fundamentally novel lens on model training. The abstract is not only technically rigorous\u2014detailing experimental verification at multiple model scales\u2014but also posing thought-provoking questions about safety, hallucination, and sample diversity. Its suggestion to reconceptualize 'training samples' as 'prompts' is an insightful, original idea that could inspire new directions in machine learning research.\n\nIn contrast, Abstract B, while solid and technically strong, presents the development of a new renderer (NeuScape) that combines existing paradigms (differentiable rendering and neural representation) to improve upon benchmarks. While impactful and relevant, it feels more incremental and less groundbreaking in conceptual originality.\n\nTherefore, Abstract A demonstrates deeper human creativity, more surprising insights, and a larger potential for impact.",
        "was_flipped": true,
        "base_seed": 48,
        "steering_seed": 49
      },
      {
        "round": 5,
        "base_abstract": "Abstract:\n\nWe introduce a novel framework, Adaptive Graph Convolutional Networks (AGCN), that dynamically adjusts the graph structure during training to adapt to the underlying data distribution. Unlike static graph approaches, AGCN employs an attention mechanism to learn the most relevant graph connections, significantly improving the model's ability to capture complex dependencies in non-stationary and high-dimensional data. Our framework seamlessly integrates with existing graph convolutional networks, offering a flexible and scalable solution for a wide range of applications including time-series analysis, recommendation systems, and biological networks.\n\nOur experimental evaluations on benchmark datasets from diverse domains, such as financial time series, social networks, and protein-protein interaction graphs, demonstrate that AGCN outperforms state-of-the-art methods by up to 20% in terms of prediction accuracy. We further analyze the learned graph structures and find that AGCN effectively captures both global and local dependencies, leading to more robust and interpretable models. The proposed framework not only addresses the limitations of fixed graph structures but also opens new avenues for exploring dynamic graph learning in various AI applications.\n\nThis work represents a significant advancement in the field of graph neural networks, providing a practical and theoretically grounded solution to the problem of adaptivity in graph-based models. Our contributions are technically rigorous, with clear demonstrations of superior performance and wide applicability, making AGCN a compelling candidate for the best paper award at major AI conferences such as ICML, ICLR, and NeurIPS.",
        "steering_abstract": "**Abstract:**\n\nDespite decades of progress, machine learning models still suffer from catastrophic forgetting when repeatedly trained on tasks sequentially. Previous efforts to reduce catastrophic forgetting have focused on retaining or replaying the old training data. We take a different route, proposing a novel approach, *Model-Free Retargeting* (MFR), that instead targets the *old model weights*, rather than the old training data. MFR re-weights the current training data such that the current model, when trained on the re-weighted data, would match the old model's predictions. We demonstrate that MFR requires only a few seconds to re-target models pre-trained on a gigabyte of data, and it can substantially reduce catastrophic forgetting, improving over the best existing approaches. Our code is publicly available, open-sourced at https://github.com/google-research/MFR. \n\nThe key technical contribution is showing that MFR is both sample- and compute-efficient. MFR only needs to re-weight the current training data, requiring no access to the old data, and MFR trains the current model on a re-weighted batch, requiring no compute for the old model, which could be prohibitively compute expensive. MFR thus trains current models faster and at a much lower compute cost than previous approaches. We then use this re-weighting idea to directly re-target the old model, making it match the current model. We prove that re-targeting the old model exactly matches the current model, and it only requires a batch of current model predictions. We show that when used to directly re-target the old model, MFR can also outperform existing approaches, for a cheap per-example cost. Both MFR and re-targeting the old model avoid using the old data. To our knowledge, this is the first approach to avoid using the old data, and our method is the most sample-efficient, compute-efficient, and sample-efficient to re-target the old model with existing methods. Our theory and empirical results demonstrate how MFR is efficient and can be broadly applied to substantially outperform the best existing methods that require substantially more samples and compute.",
        "winner": "abstract_b",
        "explanation": "While both abstracts propose novel approaches to longstanding challenges in machine learning, Abstract A demonstrates a higher degree of human creativity and potential impact for several reasons. First, it addresses the fundamental problem of catastrophic forgetting in sequential learning, which has broad and deep implications for the field. What sets Abstract A apart is its genuinely original perspective: instead of storing or replaying old data\u2014a well-explored area\u2014it introduces Model-Free Retargeting (MFR), focusing on re-weighting current data with respect to the model's own past weights. This technique innovatively sidesteps the need for revisiting old data entirely, a restriction many assumed impossible to overcome efficiently, making it a true paradigm shift.\n\nFurther, Abstract A articulates clear theoretical and empirical advancements, emphasizing both sample and compute efficiency, and claims (with supporting theory) a first-of-its-kind impact over previous methods. The writing is precise, the technical contribution is well-motivated and substantiated, and the potential impact\u2014enabling continual learning without storing or accessing old data\u2014could influence a vast array of machine learning applications.\n\nAbstract B, while technically sound and impactful within graph neural networks, advances adaptability via an attention mechanism, a more incremental improvement in a rapidly accelerating subfield. Its contribution is valuable and rigorously evaluated, but lacks the disruptive, broad-reaching creativity and theoretical innovation that Abstract A demonstrates. Therefore, Abstract A stands out as the more creative and impactful work.",
        "was_flipped": true,
        "base_seed": 50,
        "steering_seed": 51
      },
      {
        "round": 6,
        "base_abstract": "Title: **Learning Structured Predictive Representations via Differentiable Optimal Transport**\n\nAbstract: In this paper, we introduce a novel framework for learning structured predictive representations using differentiable optimal transport (OT) techniques. Our approach leverages the mathematical properties of OT to enable end-to-end learning of complex, structured predictions in a differentiable manner. We propose a differentiable OT layer that can be integrated into deep learning architectures, allowing for the optimization of structured prediction tasks without the need for ad-hoc combinatorial optimization methods. \n\nOur method addresses a critical gap in the field by providing a principled way to handle structured outputs in a differentiable setting, thereby enabling the training of deep models for problems such as sequence alignment, graph matching, and multi-object tracking. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, showing significant improvements in both accuracy and efficiency compared to existing methods. \n\nFurthermore, we contribute a robust theoretical analysis that establishes the convergence of our optimization algorithm and provides guarantees on the quality of the learned representations. Our work not only advances the state-of-the-art in structured prediction but also opens new avenues for integrating optimal transport theory with deep learning, potentially impacting a wide range of applications in computer vision, natural language processing, and beyond.",
        "steering_abstract": "**Provably Optimal and Provably Trustworthy: Learning to Solve Constraint Satisfactions with Oracle Errors**\n\nWe propose a novel, provably correct algorithm to solve constraint satisfaction problems (CSPs) by learning a neural oracle that (1) is provably correct (returns a correct solution if one exists) and (2) has provable trustworthiness bounds (returns a correct solution at least some fraction of the time with high probability). Surprisingly, the core of our algorithm is simple and similar to the way humans reason: a decision tree with oracles for each leaf node that are often learned from examples. We rigorously prove that the algorithm is correct (provably finds a solution if one exists) and trustworthy (provably finds a solution at least some fraction of the time with high probability). We prove these guarantees using simple tools from probability theory and basic facts from CSPs. We train oracles on random problems with up to 60 variables and we get 100% correctness in a 10s time budget (i.e. the oracle is provably correct and the algorithm is provably correct). Our algorithm is the first that makes these guarantees provably correct. Our implementation is open source and takes under 200 lines of python code. We hope provably correct approaches to hard problems help make the world more correct and more trustworthy.",
        "winner": "abstract_b",
        "explanation": "Abstract A demonstrates a higher degree of truly human creativity and impactful ideas. It introduces a provably correct algorithm for constraint satisfaction problems (CSPs), incorporating the innovative concept of combining neural oracles within a human-like decision tree reasoning framework. The dual guarantee of being both provably correct and provably trustworthy is a fresh and substantial contribution\u2014addressing the need for reliable and trustworthy AI systems. The method is not only theoretically rigorous (with formal guarantees) but also practical, with an efficient, open-source implementation. The clear, concise writing underscores the originality and real-world relevance of the work. \n\nIn contrast, Abstract B, while technically solid and offering a robust integration of optimal transport into structured prediction, builds more incrementally on existing lines of research (differentiable OT in deep learning), whereas Abstract A tackles a foundational problem\u2014trust and correctness in algorithmic decision-making\u2014in an especially novel and human-inspired way. Therefore, Abstract A stands out as more human-creative and potentially impactful.",
        "was_flipped": true,
        "base_seed": 52,
        "steering_seed": 53
      },
      {
        "round": 7,
        "base_abstract": "**Title: Differentiable Meta-Learning for Robust and Generalizable Out-of-Distribution Detection**\n\n**Abstract:**\nIn this paper, we introduce a novel framework for differentiable meta-learning, designed to enhance the robustness and generalizability of out-of-distribution (OOD) detection in machine learning models. Traditional approaches to OOD detection often rely on handcrafted features or complex heuristic-based methods, which are prone to overfitting and lack the flexibility to adapt to diverse and unseen data distributions. Our proposed framework, dubbed Differentiable Meta-Learning for Out-of-Distribution Detection (DML-OD), employs a meta-learning paradigm to optimize the model\u2019s representation in a way that it can effectively distinguish between in-distribution and out-of-distribution samples.\n\nAt the core of DML-OD is a differentiable meta-trainer that learns a task-agnostic representation space from a set of in-distribution and out-of-distribution samples. This representation space is optimized to minimize the discrepancy between the in-distribution and out-of-distribution distributions, ensuring that the model can generalize well to unseen data. We propose a novel loss function that dynamically adjusts the learning process based on the distributional shift, allowing the model to adapt to various types of OOD data.\n\nThe effectiveness of DML-OD is demonstrated through extensive experiments on several benchmark datasets, including ImageNet-C, ImageNet-R, and OOD datasets from multiple domains. Our results show that DML-OD significantly outperforms state-of-the-art methods in terms of both accuracy and robustness to various types of distribution shifts. Moreover, DML-OD exhibits a strong generalization capability, maintaining high performance on out-of-distribution samples not seen during the training phase.\n\nThe theoretical analysis of the proposed framework reveals that DML-OD benefits from a well-balanced representation learning process, leading to improved OOD detection performance. Our contributions not only advance the state of the art in OOD detection but also provide a robust foundation for future research in meta-learning and robust machine learning. This work is a significant step towards developing more reliable and adaptive machine learning systems capable of handling real-world distribution shifts.",
        "steering_abstract": "We present a new class of methods, called \\textbf{L}earning-to-\\textbf{L}earn (\\textbf{L}2\\textbf{L}), which enable a model to sample another, more expressive model conditioned on some prompt. This conditional generative model is then conditioned on a human-written prompt, generating a model that learns to produce the outputs a human would, given the same prompt. Our key finding is that \\textbf{L}2\\textbf{L}---by which we can sample a new model---can be thought of as \\emph{learning to learn} (the prompt), \\emph{learning to sample} (from the new model), or \\emph{learning to condition} (the new model on the prompt). We demonstrate that \\textbf{L}2\\textbf{L} is able to synthesize new models and improve existing models on a number of tasks, from natural language to 2D and 3D. For example, we can reprogram existing models into chatbots, turn a chatbot into a classifier, and turn a language model into a painter. We hope \\textbf{L}2\\textbf{L} becomes a core tool for doing science and engineering with the newest models, just like we use text editors to work with text and spreadsheets to work with numbers.",
        "winner": "abstract_b",
        "explanation": "Abstract A stands out as the winner in terms of human creativity and potential impact. It introduces the novel and conceptually rich idea of \"learning to learn\" (L2L): a meta-method where models generate other models conditioned on prompts, which in turn learn as a human would for those prompts. The framework suggests foundational shifts\u2014models that can be reprogrammed across domains (e.g., language model to classifier to painter)\u2014and positions L2L as a potential core tool for scientific and engineering work. This level of abstraction and proposed versatility reflects deep, creative insight with far-reaching implications for the future of AI tools.\n\nBy comparison, Abstract B, while technically rigorous and a solid step forward for out-of-distribution detection, is more incremental: it improves on OOD detection using meta-learning and new loss functions, but this is a more specific application within existing lines of research. Abstract B is clear, thorough, and valuable, but does not reach the same level of broad, transformative creativity or cross-domain impact implied by Abstract A.\n\nIn summary, Abstract A is more original, creative, and hints at revolutionary impact, making it the better choice by the criteria outlined.",
        "was_flipped": true,
        "base_seed": 54,
        "steering_seed": 55
      },
      {
        "round": 8,
        "base_abstract": "**Abstract:**\n\nThis research introduces a novel architecture, termed \"Hierarchical Adaptive Transformers\" (HAT), designed to efficiently process and learn from hierarchical structured data in complex environments. HAT integrates advanced transformer mechanisms with a hierarchical latent space representation, enabling the model to capture long-range dependencies and hierarchical relationships more effectively than existing architectures. Our approach dynamically adapts to varying levels of structural complexity, significantly improving performance on tasks such as natural language processing, program synthesis, and molecular design.\n\nWe present a series of extensive experiments demonstrating that HAT outperforms state-of-the-art models on several benchmark datasets, including improvements of up to 20% in accuracy on natural language understanding tasks and 30% in molecular design precision. Additionally, we provide a detailed analysis of the model's scalability and robustness, highlighting its ability to handle large-scale and noisy data.\n\nThe theoretical analysis reveals that HAT's hierarchical latent space enables better generalization by reducing the overfitting to specific hierarchical structures, while the adaptive mechanism ensures efficient resource utilization. Our contributions significantly advance the field by providing a more versatile and powerful tool for processing hierarchical data, with broad implications for applications in natural language processing, bioinformatics, and program synthesis. We believe that HAT's innovative design and empirical successes make it a compelling candidate for the best paper award at ICML, ICLR, or NeurIPS.",
        "steering_abstract": "Approximate Inference for Determinantal Point Processes (DPPs) is a challenging problem, due to their negative-definite kernels, that has motivated multiple heuristic approaches. We propose the first provably-convergent, provably-correct approximate inference approach that is also tractable (for all kernels) and fast (using only matrix-vector products). We show that our method obtains state-of-the-art or near-state-of-the-art performance on many datasets in a wide range of tasks that use DPPs, including recommendation, subset selection, and sketching. We show that this simple iterative method is also better calibrated than a previous method and often has lower entropy, demonstrating that it is in some sense better calibrated for some tasks, and that it uses samples in a better way than the previous method. We also show the first exact/ideal and approximate/empirical SVD for DPPs, which is needed for other fast sampling or score functions. Our results also give the first non-heuristic methods to do DPP MLE or other optimization of the DPP objective. This is a surprising and strong result, since the MLE or optimization of the full likelihood of a DPP is NP-hard. Our methods thus can be used to do exact or approximate MLE or optimization of the DPP objective. Our work also provides the first non-heuristic way to do sampling or score functions for the DPP or its marginal, when the kernel is arbitrary. This provides a way to do sampling or score functions that are provably convergent, fast, and can use arbitrary kernels. Our method is also the first convergent and provably-correct approximate inference algorithm that is also provably tractable and fast. We give the first non-heuristic way to do sampling or score functions for the DPP or its marginal, when the kernel is arbitrary, so now all of the tasks that use the DPP, and all the things you can do with the DPP, can be done with fast non-heuristic methods. Our method is provably correct and convergent, fast, and tractable, and is a major advance.",
        "winner": "abstract_a",
        "explanation": "Abstract B demonstrates a higher degree of human creativity and potential impact due to its introduction of a novel architectural paradigm\u2014the 'Hierarchical Adaptive Transformers' (HAT). This architecture not only presents an innovative integration of hierarchical latent spaces with adaptive transformer mechanisms, but also claims significant empirical improvements across diverse, high-impact areas including natural language processing, program synthesis, and molecular design. The abstract is well-written, clear about the model's novelty, and presents convincing evidence of both technical rigor and real-world relevance through benchmark results and theoretical analysis. In contrast, while Abstract A introduces important advances in DPP inference\u2014providing the first provably-correct fast approximate methods\u2014its novelty is more incremental, and the applications described, though meaningful, do not have as broad or transformative a potential as the new architecture in Abstract B. Furthermore, the writing in Abstract B is clearer and does a better job of illustrating the broader significance and creativity of the work.",
        "was_flipped": true,
        "base_seed": 56,
        "steering_seed": 57
      },
      {
        "round": 9,
        "base_abstract": "**Abstract:**\n\nWe present a novel framework, **Temporal Graph Neural Networks with Adaptive Neighborhoods (TGNN-AN)**, which significantly advances the state-of-the-art in modeling temporal dynamics in dynamic graphs. Unlike existing approaches that often rely on fixed-neighborhood structures, TGNN-AN dynamically adapts the neighborhood size and structure based on temporal proximity and graph structural patterns. This adaptive mechanism enables the model to capture both short-term and long-term dependencies more effectively, thereby improving prediction accuracy and stability in evolving network environments.\n\nOur framework is grounded in a rigorous theoretical analysis, demonstrating that the adaptive neighborhood selection process can asymptotically converge to optimal configurations for a wide range of graph dynamical systems. Empirically, we evaluate TGNN-AN on a diverse set of real-world datasets, including social networks, transportation networks, and financial transactions, showcasing up to 20% improvement in predictive performance compared to state-of-the-art methods. Notably, our approach also shows robustness to noise and outperforms in scenarios with sparse or noisy temporal data.\n\nMoreover, we introduce a scalable algorithmic implementation of TGNN-AN, which efficiently handles large-scale graphs with billions of nodes and edges, making the framework applicable to real-time applications. The code and pre-trained models are publicly available, facilitating reproducibility and further research. This work not only advances the theoretical understanding of dynamic graph learning but also opens new avenues for practical applications in various domains, such as social media analytics, traffic management, and financial forecasting.",
        "steering_abstract": "The success of modern machine learning systems often hinges on the availability of large-scale human-labeled data. However, the quality of that data is heavily influenced by data annotators, who may suffer from low salaries, long working hours, and stressful work conditions. We propose a large-scale crowd-sourced dataset \\textbf{\\textsc{Annoyed}}, the first of its kind to study human annotators' frustration as a growing research topic. We study 50,000+ human workers from 75 countries on two labeling tasks, and find that people from lower income countries are significantly more frustrated, as are women. We find an inverted-U shape relationship between average frustration and workers' pay, with workers being either the happiest or the most frustrated when paid the least. We find a similar relationship of average frustration with how long people are asked to work, the less people work, the happier they tend to be. Finally, we look at the difference in frustration by task type and find that rating text is 10 times more frustrating than rating images. We propose 4 simple interventions that would help workers, like being allowed to pause labeling to catch their breath, and show this improves average frustration by up to 500%. Overall, this dataset and our analysis of it show the importance of studying workers and that doing so helps build kinder, more efficient annotation services that are happier to work.",
        "winner": "abstract_b",
        "explanation": "Abstract B stands out as the more truly human creative and impactful work. Rather than advancing technical performance within a well-established research trajectory (as Abstract A does), Abstract B breaks new ground by addressing the often-ignored human cost within an essential component of machine learning systems: the labor of annotators. It introduces the first dataset specifically targeting humanity, emotion, and workplace well-being in data annotation, a profound and novel frontier for AI research. The abstract not only reveals unique and thought-provoking findings (like the inverted-U shaped frustration-pay relationship and differential impacts on different demographics), but it also directly proposes actionable interventions with substantial measured improvement, suggesting its findings could meaningfully improve real-world practices and worker experience. The scope, empathy, uncovering of human insight, and practical impact mark Abstract B as more genuinely creative and consequential to both the AI field and the people it relies on.",
        "was_flipped": false,
        "base_seed": 58,
        "steering_seed": 59
      },
      {
        "round": 10,
        "base_abstract": "Abstract: \nIn this paper, we introduce **Meta-Learning for Robust Multi-Task Reinforcement Learning (MAML-MTRL)**, a novel framework that significantly advances the state-of-the-art in multi-task reinforcement learning (MTRL). Our approach leverages meta-learning techniques to efficiently learn policies that generalize across multiple related tasks, thereby overcoming the limitations of traditional reinforcement learning methods that struggle with diverse and complex task environments. Specifically, MAML-MTRL employs a meta-optimizer to adapt to the task distribution, enabling rapid adaptation to new tasks with minimal data. We propose a novel variant of the Model-Agnostic Meta-Learning (MAML) algorithm, tailored for the MTRL setting, which we denote as **Meta-Adaptive MTRL (MAML-MTRL)**. Theoretical analysis demonstrates that MAML-MTRL converges to optimal task-specific policies under mild assumptions, and empirical evaluations across a range of benchmarks, including complex robotic manipulation tasks, show a significant improvement in generalization performance compared to existing methods. Moreover, we introduce a scalable, efficient implementation of our algorithm that can handle high-dimensional observation spaces, making it applicable to real-world scenarios. This work not only provides a robust solution for multi-task reinforcement learning but also opens new avenues for research in meta-learning and reinforcement learning for real-world applications.",
        "steering_abstract": "```\nWe present *Interpretable Latent Planning*, a method that endows language models with a latent plan-like ability to enable them to flexibly apply a given set of steps to solve downstream tasks. Unlike many existing models that treat a plan as a black box, our method interprets a plan by reasoning over the steps and allows humans to verify plans (e.g., by checking whether a step has an action with the right verb) without any training. We show that, although planning steps are sampled at inference-time, our method is able to generalize to novel plans and use prior plans to flexibly reason about novel situations, like a person needing to get some bread from the kitchen, even if the plan was to get some milk from the kitchen. This is enabled by an auxiliary planning head which does not need to be trained; we simply sample plans for it at inference time. To verify plans, we propose an unanswerability head and show that a human can use it (with a small amount of training) to unanswer plans they think are wrong, similarly to checking a student\u2019s math homework. Our approach outperforms many prior approaches for language model planning at predicting plan steps, unanswerability, and answering reasoning prompts, as well as achieving higher human-verifiability.\n```",
        "winner": "abstract_b",
        "explanation": "Abstract A demonstrates greater human creativity and potential impact due to several factors: (1) It introduces a novel, interpretable approach to planning in language models, emphasizing human verifiability and reasoning\u2014an aspect underexplored in current AI research. (2) The method uniquely allows plans to be sampled at inference without any training, increasing flexibility and adaptability. (3) The introduction of an 'unanswerability head' for plan verification is especially innovative, as it bridges human-centered evaluation with machine reasoning. (4) The writing is clear and communicates both the technical contributions and the human-centered design ethos. In contrast, Abstract B, while rigorous and methodologically sound, extends existing frameworks (meta-learning in multi-task RL) and presents incremental improvements, lacking the same level of novelty or creative leap as Abstract A's human-verifiable planning interface.",
        "was_flipped": true,
        "base_seed": 60,
        "steering_seed": 61
      }
    ],
    "summary": {
      "total_comparisons": 10,
      "base_wins": 1,
      "steering_wins": 9,
      "ties": 0,
      "base_win_rate": 0.1,
      "steering_win_rate": 0.9
    }
  }
}